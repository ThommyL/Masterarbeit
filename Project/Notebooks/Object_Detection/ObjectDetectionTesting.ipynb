{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook is undocumented (and not cleanly coded) since it just gathers resources for the pdf document. Please refer to said document for results and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os.path\n",
    "import pickle\n",
    "import random\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "\n",
    "from Project.AutoSimilarityCacheConfiguration.DataAccess import DataAccess\n",
    "from Project.Utils.ConfigurationUtils.ConfigLoader import ConfigLoader\n",
    "from Project.Utils.Misc.OriginContainer import OriginContainer\n",
    "from Project.VisualFeaturesBranches.ObjectDetection.ArtDataset import ArtDataset\n",
    "from Project.VisualFeaturesBranches.ObjectDetection.DatasetSplits import DatasetSplits\n",
    "from Project.VisualFeaturesBranches.ObjectDetection.ObjectDetectionNetworkUtils import collate, get_pretrained_model, train_one_epoch, load_model, reduce_box_count, get_winning_config_with_augmentations, get_winning_config_without_augmentations\n",
    "\n",
    "cl: ConfigLoader = ConfigLoader.instance\n",
    "cl.data_cleaning_in_progress = True  # Prohibits cache generation in DataAccess class\n",
    "\n",
    "da: DataAccess = DataAccess.instance\n",
    "\n",
    "check_ids_tree: List[str] = []\n",
    "for i in da.get_ids_for_which_tree_annotation_exists():\n",
    "    if i not in da.get_ids_for_which_bounding_boxes_exist():\n",
    "        check_ids_tree.append(i)\n",
    "\n",
    "check_ids_person: List[str] = []\n",
    "for i in da.get_ids_for_which_person_annotation_exists():\n",
    "    if i not in da.get_ids_for_which_bounding_boxes_exist():\n",
    "        check_ids_person.append(i)\n",
    "\n",
    "if not os.path.exists('result_dict_trees.pkl'):\n",
    "    with open('result_dict_trees.pkl', 'wb+') as f:\n",
    "        pickle.dump(dict(), f)\n",
    "if not os.path.exists('result_dict_persons.pkl'):\n",
    "    with open('result_dict_persons.pkl', 'wb+') as f:\n",
    "        pickle.dump(dict(), f)\n",
    "with open('result_dict_trees.pkl', 'rb') as f:\n",
    "    tree_result_dict = pickle.load(f)\n",
    "with open('result_dict_persons.pkl', 'rb') as f:\n",
    "    persons_result_dict = pickle.load(f)\n",
    "\n",
    "number_of_trainable_layers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset_splits():\n",
    "    return DatasetSplits(1200, 0, 0, 0, 0, 0, (0, 0), 0, False)\n",
    "\n",
    "def get_art_dataset(ids):\n",
    "    return ArtDataset(ids, 1200, 1200, 0, 0, 0, 0, 0, 0, (0, 0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dss: DatasetSplits = get_dataset_splits()\n",
    "len_train_set = sum(lengths := [len(ds) for ds in [dss.get_train(nr, False, None) for nr in range(dss.number_of_splits)]])/len(lengths)\n",
    "len_test_set = sum(lengths := [len(ds) for ds in [dss.get_test(nr) for nr in range(dss.number_of_splits)]])/len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dss: DatasetSplits = get_dataset_splits()\n",
    "\n",
    "trees_in_train_positive = 0\n",
    "trees_in_train_negative = 0\n",
    "trees_in_test_positive = 0\n",
    "trees_in_test_negative = 0\n",
    "trees_in_all_positive = 0\n",
    "trees_in_all_negative = 0\n",
    "\n",
    "trees_in_train_splits_positive = []\n",
    "trees_in_train_splits_negative = []\n",
    "trees_in_test_splits_positive = []\n",
    "trees_in_test_splits_negative = []\n",
    "trees_in_all_splits_positive = []\n",
    "trees_in_all_splits_negative = []\n",
    "\n",
    "\n",
    "for nr in range(dss.number_of_splits):\n",
    "    trees_in_train_splits_positive.append(0)\n",
    "    trees_in_train_splits_negative.append(0)\n",
    "    trees_in_test_splits_positive.append(0)\n",
    "    trees_in_test_splits_negative.append(0)\n",
    "    for e in iter(dss.get_train(nr, False, None)):\n",
    "        if 'Baum' in [da.get_class_label_for_index(l) for l in e[1]['labels']]:\n",
    "            trees_in_train_positive += 1\n",
    "            trees_in_train_splits_positive[nr] += 1\n",
    "        else:\n",
    "            trees_in_train_negative += 1\n",
    "            trees_in_train_splits_negative[nr] += 1\n",
    "    for e in iter(dss.get_test(nr)):\n",
    "        if 'Baum' in [da.get_class_label_for_index(l) for l in e[1]['labels']]:\n",
    "            trees_in_test_positive += 1\n",
    "            trees_in_test_splits_positive[nr] += 1\n",
    "        else:\n",
    "            trees_in_test_negative += 1\n",
    "            trees_in_test_splits_negative[nr] += 1\n",
    "\n",
    "trees_in_test_positive /= dss.number_of_splits\n",
    "trees_in_test_negative /= dss.number_of_splits\n",
    "trees_in_train_positive /= dss.number_of_splits\n",
    "trees_in_train_negative /= dss.number_of_splits\n",
    "\n",
    "for e in iter(dss.get_training_all(0, 1)):\n",
    "    if 'Baum' in [da.get_class_label_for_index(l) for l in e[1]['labels']]:\n",
    "        trees_in_all_positive += 1\n",
    "    else:\n",
    "        trees_in_all_negative += 1\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "for c in check_ids_tree:\n",
    "    if da.get_tree_annotation_for_identifier(c):\n",
    "        positive += 1\n",
    "    else:\n",
    "        negative += 1\n",
    "\n",
    "trees_in_train_splits_positive = [p / len_train_set for p in trees_in_train_splits_positive]\n",
    "trees_in_train_splits_negative = [p / len_train_set for p in trees_in_train_splits_negative]\n",
    "trees_in_test_splits_positive = [p / len_test_set for p in trees_in_test_splits_positive]\n",
    "trees_in_test_splits_negative = [p / len_test_set for p in trees_in_test_splits_negative]\n",
    "\n",
    "print(\n",
    "    f'tree annotations: {positive / (positive + negative)}\\ntraining set: {trees_in_train_positive / (trees_in_train_positive + trees_in_train_negative)}\\nvalidation set: {trees_in_test_positive / (trees_in_test_positive + trees_in_test_negative)}\\nall training: {trees_in_all_positive / (trees_in_all_positive + trees_in_all_negative)}\\n\\nAll splits: {trees_in_train_splits_positive=}, {trees_in_train_splits_negative=}, {trees_in_test_splits_positive=}, {trees_in_test_splits_negative=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dss: DatasetSplits = get_dataset_splits()\n",
    "\n",
    "persons_in_train_positive = 0\n",
    "persons_in_train_negative = 0\n",
    "persons_in_test_positive = 0\n",
    "persons_in_test_negative = 0\n",
    "persons_in_all_positive = 0\n",
    "persons_in_all_negative = 0\n",
    "\n",
    "persons_in_train_splits_positive = []\n",
    "persons_in_train_splits_negative = []\n",
    "persons_in_test_splits_positive = []\n",
    "persons_in_test_splits_negative = []\n",
    "persons_in_all_splits_positive = []\n",
    "persons_in_all_splits_negative = []\n",
    "\n",
    "for nr in range(dss.number_of_splits):\n",
    "    persons_in_train_splits_positive.append(0)\n",
    "    persons_in_train_splits_negative.append(0)\n",
    "    persons_in_test_splits_positive.append(0)\n",
    "    persons_in_test_splits_negative.append(0)\n",
    "\n",
    "    for e in iter(dss.get_train(nr, False, None)):\n",
    "        if 'Person' in [da.get_class_label_for_index(l) for l in e[1]['labels']]:\n",
    "            persons_in_train_positive += 1\n",
    "            persons_in_train_splits_positive[nr] += 1\n",
    "        else:\n",
    "            persons_in_train_negative += 1\n",
    "            persons_in_train_splits_negative[nr] += 1\n",
    "    for e in iter(dss.get_test(nr)):\n",
    "        if 'Person' in [da.get_class_label_for_index(l) for l in e[1]['labels']]:\n",
    "            persons_in_test_positive += 1\n",
    "            persons_in_test_splits_positive[nr] += 1\n",
    "        else:\n",
    "            persons_in_test_negative += 1\n",
    "            persons_in_test_splits_negative[nr] += 1\n",
    "\n",
    "persons_in_test_positive /= dss.number_of_splits\n",
    "persons_in_test_negative /= dss.number_of_splits\n",
    "persons_in_train_positive /= dss.number_of_splits\n",
    "persons_in_train_negative /= dss.number_of_splits\n",
    "\n",
    "for e in iter(dss.get_training_all(0, 1)):\n",
    "    if 'Person' in [da.get_class_label_for_index(l) for l in e[1]['labels']]:\n",
    "        persons_in_all_positive += 1\n",
    "    else:\n",
    "        persons_in_all_negative += 1\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "for c in check_ids_person:\n",
    "    if da.get_person_annotation_for_identifier(c):\n",
    "        positive += 1\n",
    "    else:\n",
    "        negative += 1\n",
    "\n",
    "persons_in_train_splits_positive = [p / len_train_set for p in persons_in_train_splits_positive]\n",
    "persons_in_train_splits_negative = [p / len_train_set for p in persons_in_train_splits_negative]\n",
    "persons_in_test_splits_positive = [p / len_test_set for p in persons_in_test_splits_positive]\n",
    "persons_in_test_splits_negative = [p / len_test_set for p in persons_in_test_splits_negative]\n",
    "\n",
    "print(\n",
    "    f'person annotations: {positive / (positive + negative)}\\ntraining set: {persons_in_train_positive / (persons_in_train_positive + persons_in_train_negative)}\\nvalidation set: {persons_in_test_positive / (persons_in_test_positive + persons_in_test_negative)}\\nall training: {persons_in_all_positive / (persons_in_all_positive + persons_in_all_negative)}\\n\\nAll splits: {persons_in_train_splits_positive=}, {persons_in_train_splits_negative=}, {persons_in_test_splits_positive=}, {persons_in_test_splits_negative=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_count = 0\n",
    "while True:\n",
    "    if os.path.exists(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'VisualFeaturesBranches',\n",
    "                                   'RegionOfInterestExtractor', 'Generated_Files', 'State_Dictionary',\n",
    "                                   f'best_config_grid_{model_count + 1}_model.pkl')):\n",
    "        model_count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "def get_stats(from_model, ds, device, score_threshold, for_tree, nms=0.4):\n",
    "    if for_tree:\n",
    "        check_ids = check_ids_tree\n",
    "    else:\n",
    "        check_ids = check_ids_person\n",
    "    from_model.to(device)\n",
    "    from_model.eval()\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    identifiers_false_positive = []\n",
    "    identifiers_false_negative = []\n",
    "\n",
    "    for ident in (loading_bar := tqdm(check_ids, total=len(check_ids))):\n",
    "        stats_img, _ = ds.get_item_by_identifier(ident)\n",
    "        stats_output = from_model([stats_img.to(device)])\n",
    "        stats_reduced = reduce_box_count(stats_output[0], nms, score_threshold, 0)\n",
    "        stats_labels = [da.get_class_label_for_index(l) for l in stats_reduced['labels']]\n",
    "\n",
    "        del stats_img\n",
    "        del stats_output\n",
    "        del stats_reduced\n",
    "        gc.collect()\n",
    "\n",
    "        if for_tree:\n",
    "            found = 'Baum' in stats_labels\n",
    "            exists = da.get_tree_annotation_for_identifier(ident)\n",
    "        else:\n",
    "            found = 'Person' in stats_labels\n",
    "            exists = da.get_person_annotation_for_identifier(ident)\n",
    "\n",
    "        if exists:\n",
    "            pos += 1\n",
    "            if found:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "                identifiers_false_negative.append(ident)\n",
    "        else:\n",
    "            neg += 1\n",
    "            if found:\n",
    "                false_positive += 1\n",
    "                identifiers_false_positive.append(ident)\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        loading_bar.set_description(\n",
    "            f'{true_positive=}, {false_positive=}, {true_negative=}, {false_negative=}, {pos=}, {neg=}')\n",
    "    return true_positive, false_positive, true_negative, false_negative, pos, neg, identifiers_false_positive, identifiers_false_negative\n",
    "\n",
    "\n",
    "for experiment, config in zip(['Without_Augmentations', 'With_Augmentations'], [get_winning_config_without_augmentations()[0], get_winning_config_with_augmentations()[0]]):\n",
    "    if experiment in tree_result_dict.keys() and experiment in persons_result_dict.keys():\n",
    "        continue\n",
    "    print(f'Testing model from experiment {experiment}')\n",
    "\n",
    "    model = load_model(experiment)\n",
    "\n",
    "    threshold = 0.4\n",
    "\n",
    "    if experiment not in tree_result_dict.keys():\n",
    "        tree_result_dict[experiment] = (get_stats(model, get_art_dataset(check_ids_tree), config['device'], threshold, True))\n",
    "    if experiment not in persons_result_dict.keys():\n",
    "        persons_result_dict[experiment] = (\n",
    "            get_stats(model, get_art_dataset(check_ids_person), config['device'], threshold, False))\n",
    "    del model\n",
    "    gc.collect()\n",
    "    with open('result_dict_trees.pkl', 'wb+') as f:\n",
    "        pickle.dump(tree_result_dict, f)\n",
    "    with open('result_dict_persons.pkl', 'wb+') as f:\n",
    "        pickle.dump(persons_result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix_entry, name):\n",
    "    true_positive = matrix_entry[0]\n",
    "    false_positive = matrix_entry[1]\n",
    "    true_negative = matrix_entry[2]\n",
    "    false_negative = matrix_entry[3]\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    plt.subplots()\n",
    "    sn.heatmap(pd.DataFrame([[true_positive, false_positive], [false_negative, true_negative]],\n",
    "                            index=['Predicted Positive', 'Predicted Negative'],\n",
    "                            columns=['True Positive', 'True Negative']), annot=True, vmin=0, vmax=1000, fmt=\"\",\n",
    "               annot_kws={'weight': 'bold', 'size': 'large'})\n",
    "    plt.title(f'F1: {f1:1.3f}, Precision: {precision:1.3f},  Recall: {recall:1.3f}', fontdict={'fontsize': 14})\n",
    "\n",
    "    plt.savefig(f'confusion_matrix_{name}.png')\n",
    "\n",
    "for k in tree_result_dict.keys():\n",
    "    entry = tree_result_dict[k]\n",
    "    plot_confusion_matrix(entry, f'{k}_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k in persons_result_dict.keys():\n",
    "    entry = persons_result_dict[k]\n",
    "    plot_confusion_matrix(entry, f'{k}_person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_experiment = 'With_Augmentations'\n",
    "best_model = load_model(best_experiment)\n",
    "best_config = get_winning_config_with_augmentations()[0]\n",
    "best_size = best_config['size']\n",
    "best_horizontal_flip_p = best_config['p_horizontal_flip']\n",
    "best_vertical_flip_p = best_config['p_vertical_flip']\n",
    "best_brightness_jitter = best_config['brightness_jitter']\n",
    "best_contrast_jitter = best_config['contrast_jitter']\n",
    "best_saturation_jitter = best_config['saturation_jitter']\n",
    "best_hue_jitter = best_config['hue_jitter']\n",
    "best_p_grayscale = best_config['p_grayscale']\n",
    "\n",
    "\n",
    "def get_training_data_set(percentage, seed):\n",
    "    data_access: DataAccess = DataAccess.instance\n",
    "    identifiers = list(data_access.get_ids_for_which_bounding_boxes_exist())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(identifiers)\n",
    "    identifiers = identifiers[:int(len(identifiers) * percentage)]\n",
    "    return identifiers\n",
    "\n",
    "\n",
    "def evaluate_model_for_dataset(model_to_evaluate, score_threshold, device, for_tree, nms):\n",
    "    if for_tree:\n",
    "        check_ids = check_ids_tree\n",
    "    else:\n",
    "        check_ids = check_ids_person\n",
    "    stats = get_stats(model_to_evaluate, get_art_dataset(check_ids), device, score_threshold, for_tree, nms=nms)\n",
    "    true_positive = stats[0]\n",
    "    false_positive = stats[1]\n",
    "    false_negative = stats[3]\n",
    "    if (true_positive + false_positive) == 0 or (true_positive + false_negative) == 0:\n",
    "        return 0\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def get_model_for_evaluation(configuration, training_ids, train_for_epochs):\n",
    "    lr, weight_decay, trainable_layers, size, p_horizontal_flip, p_vertical_flip, brightness_jitter, saturation_jitter, hue_jitter, p_grayscale, batch_size, num_workers, device, average_span, nr_epochs, report, small_model, curriculum = configuration['lr'], configuration['weight_decay'], configuration['trainable_layers'], configuration['size'], configuration['p_horizontal_flip'], configuration['p_vertical_flip'], configuration['brightness_jitter'], configuration['saturation_jitter'], configuration['hue_jitter'], configuration['p_grayscale'], configuration['batch_size'], configuration['num_workers'], configuration['device'], configuration['average_span'], configuration['nr_epochs'], configuration['report'],  configuration['small_model'], configuration['curriculum']\n",
    "\n",
    "    model_to_eval = get_pretrained_model(trainable_layers, device, small_model)\n",
    "    model_to_eval.train()\n",
    "\n",
    "    if len(training_ids) != 0:\n",
    "        optimizer = torch.optim.Adam(params=[param for param in model_to_eval.parameters() if param.requires_grad], lr=lr,\n",
    "                                     weight_decay=weight_decay)\n",
    "\n",
    "        for epoch in range(train_for_epochs):\n",
    "            factor = 1\n",
    "            if curriculum:\n",
    "                factor = epoch / train_for_epochs\n",
    "            training_dataset = ArtDataset(training_ids, width=best_size, height=best_size, for_training=True,\n",
    "                          p_horizontal_flip=best_horizontal_flip_p * factor, p_vertical_flip=best_vertical_flip_p * factor,\n",
    "                          brightness_jitter=best_brightness_jitter * factor, contrast_jitter=best_contrast_jitter * factor,\n",
    "                          saturation_jitter=best_saturation_jitter * factor, hue_jitter=best_hue_jitter * factor,\n",
    "                          p_grayscale=best_p_grayscale * factor)\n",
    "            training_loader = DataLoader(training_dataset, batch_size=batch_size, collate_fn=collate,\n",
    "                                         shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "            model_to_eval = train_one_epoch(model_to_eval, optimizer, training_loader, epoch, device, average_span, report)\n",
    "            del training_dataset\n",
    "            gc.collect()\n",
    "    return model_to_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('best_nms_and_result_tree.pkl'):\n",
    "    best_nms_tree = None\n",
    "    best_result_tree = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 21)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(best_model, score_threshold=0, device=best_config['device'],\n",
    "                                                   for_tree=True, nms=t)\n",
    "        if current_score > best_result_tree:\n",
    "            best_result_tree = current_score\n",
    "            best_nms_tree = t\n",
    "    with open('best_nms_and_result_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_nms_tree, best_result_tree), f)\n",
    "else:\n",
    "    with open('best_nms_and_result_tree.pkl', 'rb') as f:\n",
    "        best_nms_tree, best_result_tree = pickle.load(f)\n",
    "print(f'{best_nms_tree=}, {best_result_tree=}')\n",
    "\n",
    "if not os.path.exists('best_threshold_and_result_tree.pkl'):\n",
    "    best_threshold_tree = None\n",
    "    best_result_tree = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 10)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(best_model, score_threshold=t, device=best_config['device'],\n",
    "                                                   for_tree=True, nms=best_nms_tree)\n",
    "        if current_score > best_result_tree:\n",
    "            best_result_tree = current_score\n",
    "            best_threshold_tree = t\n",
    "    with open('best_threshold_and_result_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_threshold_tree, best_result_tree), f)\n",
    "else:\n",
    "    with open('best_threshold_and_result_tree.pkl', 'rb') as f:\n",
    "        best_threshold_tree, best_result_tree = pickle.load(f)\n",
    "print(f'{best_threshold_tree=}, {best_result_tree=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('best_nms_and_result_person.pkl'):\n",
    "    best_nms_person = None\n",
    "    best_result_person = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 21)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(best_model, score_threshold=0, device=best_config['device'],\n",
    "                                                   for_tree=False, nms=t)\n",
    "        if current_score > best_result_person:\n",
    "            best_result_person = current_score\n",
    "            best_nms_person = t\n",
    "    with open('best_nms_and_result_person.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_nms_person, best_result_person), f)\n",
    "else:\n",
    "    with open('best_nms_and_result_person.pkl', 'rb') as f:\n",
    "        best_nms_person, best_result_person = pickle.load(f)\n",
    "print(f'{best_nms_person=}, {best_result_person=}')\n",
    "\n",
    "if not os.path.exists('best_threshold_and_result_person.pkl'):\n",
    "    best_threshold_person = None\n",
    "    best_result_person = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 13)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(best_model, score_threshold=t, device=best_config['device'],\n",
    "                                                   for_tree=False, nms=best_nms_person)\n",
    "        if current_score > best_result_person:\n",
    "            best_result_person = current_score\n",
    "            best_threshold_person = t\n",
    "    with open('best_threshold_and_result_person.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_threshold_person, best_result_person), f)\n",
    "else:\n",
    "    with open('best_threshold_and_result_person.pkl', 'rb') as f:\n",
    "        best_threshold_person, best_result_person = pickle.load(f)\n",
    "print(f'{best_threshold_person=}, {best_result_person=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_manual(tp, fp, tn, fn, save_as_name):\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    sn.heatmap(pd.DataFrame([[tp, fp], [fn, tn]], index=['Predicted Positive', 'Predicted Negative'],\n",
    "                            columns=['True Positive', 'True Negative']), annot=True, vmin=0, vmax=1000, fmt=\"\",\n",
    "               annot_kws={'weight': 'bold', 'size': 'large'})\n",
    "    plt.title(f'F1: {f1:1.3f}, Precision: {precision:1.3f},  Recall: {recall:1.3f}', fontdict={'fontsize': 14})\n",
    "\n",
    "    plt.savefig(f'{save_as_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('best_model_stats_tree.pkl'):\n",
    "    with open('best_model_stats_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump(get_stats(best_model, get_art_dataset(check_ids_tree), torch.device('cuda'), best_threshold_tree, True, best_nms_tree), f)\n",
    "with open('best_model_stats_tree.pkl', 'rb') as f:\n",
    "    best_model_stats_tree = pickle.load(f)\n",
    "if not os.path.exists('best_model_stats_person.pkl'):\n",
    "    with open('best_model_stats_person.pkl', 'wb+') as f:\n",
    "        pickle.dump(get_stats(best_model, get_art_dataset(check_ids_person), torch.device('cuda'), best_threshold_person, False, best_nms_person), f)\n",
    "with open('best_model_stats_person.pkl', 'rb') as f:\n",
    "    best_model_stats_person = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(best_model_stats_tree[0], best_model_stats_tree[1], best_model_stats_tree[2], best_model_stats_tree[3], 'best_tree_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(best_model_stats_person[0], best_model_stats_person[1], best_model_stats_person[2], best_model_stats_person[3], 'best_person_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del best_model\n",
    "gc.collect()\n",
    "best_nr_of_epochs = get_winning_config_with_augmentations()[1]\n",
    "\n",
    "if not os.path.exists('tree_scores_dict.pkl'):\n",
    "    with open('tree_scores_dict.pkl', 'wb+') as f:\n",
    "        pickle.dump(dict(), f)\n",
    "if not os.path.exists('person_scores_dict.pkl'):\n",
    "    with open('person_scores_dict.pkl', 'wb+') as f:\n",
    "        pickle.dump(dict(), f)\n",
    "\n",
    "with open('tree_scores_dict.pkl', 'rb') as f:\n",
    "    tree_scores_dict = pickle.load(f)\n",
    "with open('person_scores_dict.pkl', 'rb') as f:\n",
    "    person_scores_dict = pickle.load(f)\n",
    "\n",
    "for p in [0.1 * i for i in range(0, 11)]:\n",
    "    if p not in tree_scores_dict.keys() or p not in person_scores_dict.keys():\n",
    "        print(f'testing with with {p}')\n",
    "        tree_scores = []\n",
    "        person_scores = []\n",
    "\n",
    "        for s in range(5):\n",
    "            current_model = get_model_for_evaluation(best_config, get_training_data_set(p, s), best_nr_of_epochs)\n",
    "            tree_scores.append(evaluate_model_for_dataset(current_model, best_threshold_tree, best_config['device'], True, best_nms_tree))\n",
    "            person_scores.append(evaluate_model_for_dataset(current_model, best_threshold_person, best_config['device'], False, best_nms_person))\n",
    "            del current_model\n",
    "            gc.collect()\n",
    "        tree_scores_dict[p] = sum(tree_scores) / len(tree_scores)\n",
    "        person_scores_dict[p] = sum(person_scores) / len(person_scores)\n",
    "\n",
    "        with open('tree_scores_dict.pkl', 'wb+') as f:\n",
    "            pickle.dump(tree_scores_dict, f)\n",
    "        with open('person_scores_dict.pkl', 'wb+') as f:\n",
    "            pickle.dump(person_scores_dict, f)\n",
    "\n",
    "print(tree_scores_dict)\n",
    "print(person_scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for current, plot_title, save_as in zip([tree_scores_dict, person_scores_dict],\n",
    "                                        ['Tree Dataset F1 Scores for Model trained on x Percent of the Data',\n",
    "                                         'Person Dataset F1 Scores for Model trained on x Percent of the Data'],\n",
    "                                        ['tree_f1_scores_for_small_dataset.png',\n",
    "                                         'person_f1_scores_for_small_dataset.png']):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.bar(list(round(k, 1) * 10 for k in current.keys()), current.values())\n",
    "    title = plot_title\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Percentage of Data')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_xticks(list(round(k, 1) * 10 for k in current.keys()), labels=[round(k, 1) for k in current.keys()])\n",
    "    plt.savefig(save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da: DataAccess = DataAccess.instance\n",
    "all_iconclass_labels = set()\n",
    "\n",
    "both_positive = 0\n",
    "only_iconclass_positive = 0\n",
    "only_ground_truth_positive = 0\n",
    "both_negative = 0\n",
    "\n",
    "for i in tqdm(check_ids_tree):\n",
    "    tree_in_iconclass = False\n",
    "\n",
    "    for tag in ['Wald', 'Baumgruppen', 'Laubwald', 'Bäume', 'Lichtung im Wald', 'Waldrand', 'Waldweg']:\n",
    "        if tag in da.get_iconclass_tags_from_identifier(i):\n",
    "            tree_in_iconclass = True\n",
    "            break\n",
    "\n",
    "    ground_truth = da.get_tree_annotation_for_identifier(i)\n",
    "\n",
    "    if tree_in_iconclass and ground_truth:\n",
    "        both_positive += 1\n",
    "    elif tree_in_iconclass and not ground_truth:\n",
    "        only_iconclass_positive += 1\n",
    "    elif ground_truth and not tree_in_iconclass:\n",
    "        only_ground_truth_positive += 1\n",
    "    elif not tree_in_iconclass and not ground_truth:\n",
    "        both_negative += 1\n",
    "hit = both_positive + both_negative\n",
    "miss = only_iconclass_positive + only_ground_truth_positive\n",
    "print(f'{both_positive=}, {only_iconclass_positive=}, {only_ground_truth_positive=}, {both_negative=}, {hit=}, {miss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da: DataAccess = DataAccess.instance\n",
    "all_iconclass_labels = set()\n",
    "\n",
    "both_positive = 0\n",
    "only_tag_positive = 0\n",
    "only_ground_truth_positive = 0\n",
    "both_negative = 0\n",
    "\n",
    "for i in tqdm(check_ids_tree):\n",
    "    tree_in_tags = 'Baum' in [t[0] for t in da.get_tag_tuples_from_identifier(i, OriginContainer(('Obj',)))]\n",
    "\n",
    "    ground_truth = da.get_tree_annotation_for_identifier(i)\n",
    "\n",
    "    if tree_in_tags and ground_truth:\n",
    "        both_positive += 1\n",
    "    elif tree_in_tags and not ground_truth:\n",
    "        only_tag_positive += 1\n",
    "    elif ground_truth and not tree_in_tags:\n",
    "        only_ground_truth_positive += 1\n",
    "    elif not tree_in_tags and not ground_truth:\n",
    "        both_negative += 1\n",
    "hit = both_positive + both_negative\n",
    "miss = only_tag_positive + only_ground_truth_positive\n",
    "print(f'{both_positive=}, {only_tag_positive=}, {only_ground_truth_positive=}, {both_negative=}, {hit=}, {miss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da: DataAccess = DataAccess.instance\n",
    "all_iconclass_labels = set()\n",
    "\n",
    "both_positive = 0\n",
    "only_prediction_positive = 0\n",
    "only_ground_truth_positive = 0\n",
    "both_negative = 0\n",
    "\n",
    "dataset = get_art_dataset(check_ids_tree)\n",
    "\n",
    "model = load_model('With_Augmentations')\n",
    "model.eval()\n",
    "\n",
    "for identifier in check_ids_tree:\n",
    "    img, targets = dataset.get_item_by_identifier(identifier)\n",
    "    output = model([img.to(torch.device('cuda'))])\n",
    "    reduced = reduce_box_count(output[0], best_nms_tree, best_threshold_tree, 0)\n",
    "\n",
    "    del img\n",
    "    del output\n",
    "    gc.collect()\n",
    "\n",
    "    labels = [da.get_class_label_for_index(l) for l in reduced['labels']]\n",
    "    tree_in_prediction = 'Baum' in labels\n",
    "\n",
    "    ground_truth = da.get_tree_annotation_for_identifier(identifier)\n",
    "\n",
    "    if tree_in_prediction and ground_truth:\n",
    "        both_positive += 1\n",
    "    elif tree_in_prediction and not ground_truth:\n",
    "        only_prediction_positive += 1\n",
    "    elif ground_truth and not tree_in_prediction:\n",
    "        only_ground_truth_positive += 1\n",
    "    elif not tree_in_prediction and not ground_truth:\n",
    "        both_negative += 1\n",
    "hit = both_positive + both_negative\n",
    "miss = only_prediction_positive + only_ground_truth_positive\n",
    "print(f'{both_positive=}, {only_prediction_positive=}, {only_ground_truth_positive=}, {both_negative=}, {hit=}, {miss=}')\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "untrained_model = get_pretrained_model(number_of_trainable_layers, torch.device('cuda'), True)\n",
    "untrained_model.train()\n",
    "\n",
    "if not os.path.exists('best_nms_and_result_untrained_person.pkl'):\n",
    "    best_nms_untrained_person = None\n",
    "    best_result_untrained_person = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 21)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(untrained_model, score_threshold=0, device=best_config['device'],\n",
    "                                                   for_tree=False, nms=t)\n",
    "        if current_score > best_result_untrained_person:\n",
    "            best_result_untrained_person = current_score\n",
    "            best_nms_untrained_person = t\n",
    "    with open('best_nms_and_result_untrained_person.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_nms_untrained_person, best_result_untrained_person), f)\n",
    "else:\n",
    "    with open('best_nms_and_result_untrained_person.pkl', 'rb') as f:\n",
    "        best_nms_untrained_person, best_result_untrained_person = pickle.load(f)\n",
    "print(f'{best_nms_untrained_person=}, {best_result_untrained_person=}')\n",
    "\n",
    "if not os.path.exists('best_threshold_and_result_untrained_person.pkl'):\n",
    "    best_threshold_untrained_person = None\n",
    "    best_result_untrained_person = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 11)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(untrained_model, score_threshold=t, device=best_config['device'],\n",
    "                                                   for_tree=False, nms=best_nms_untrained_person)\n",
    "        if current_score > best_result_untrained_person:\n",
    "            best_result_untrained_person = current_score\n",
    "            best_threshold_untrained_person = t\n",
    "    with open('best_threshold_and_result_untrained_person.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_threshold_untrained_person, best_result_untrained_person), f)\n",
    "else:\n",
    "    with open('best_threshold_and_result_untrained_person.pkl', 'rb') as f:\n",
    "        best_threshold_untrained_person, best_result_untrained_person = pickle.load(f)\n",
    "print(f'{best_threshold_untrained_person=}, {best_result_untrained_person=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "untrained_model = get_pretrained_model(number_of_trainable_layers, torch.device('cuda'), True)\n",
    "untrained_model.train()\n",
    "\n",
    "if not os.path.exists('best_nms_and_result_untrained_tree.pkl'):\n",
    "    best_nms_untrained_tree = None\n",
    "    best_result_untrained_tree = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 21)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(untrained_model, score_threshold=0, device=best_config['device'],\n",
    "                                                   for_tree=True, nms=t)\n",
    "        if current_score > best_result_untrained_tree:\n",
    "            best_result_untrained_tree = current_score\n",
    "            best_nms_untrained_tree = t\n",
    "    with open('best_nms_and_result_untrained_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_nms_untrained_tree, best_result_untrained_tree), f)\n",
    "else:\n",
    "    with open('best_nms_and_result_untrained_tree.pkl', 'rb') as f:\n",
    "        best_nms_untrained_tree, best_result_untrained_tree = pickle.load(f)\n",
    "print(f'{best_nms_untrained_tree=}, {best_result_untrained_tree=}')\n",
    "\n",
    "if not os.path.exists('best_threshold_and_result_untrained_tree.pkl'):\n",
    "    best_threshold_untrained_tree = None\n",
    "    best_result_untrained_tree = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 11)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(untrained_model, score_threshold=t, device=best_config['device'],\n",
    "                                                   for_tree=True, nms=best_nms_untrained_tree)\n",
    "        if current_score > best_result_untrained_tree:\n",
    "            best_result_untrained_tree = current_score\n",
    "            best_threshold_untrained_tree = t\n",
    "    with open('best_threshold_and_result_untrained_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_threshold_untrained_tree, best_result_untrained_tree), f)\n",
    "else:\n",
    "    with open('best_threshold_and_result_untrained_tree.pkl', 'rb') as f:\n",
    "        best_threshold_untrained_tree, best_result_untrained_tree = pickle.load(f)\n",
    "print(f'{best_threshold_untrained_tree=}, {best_result_untrained_tree=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('untrained_model_stats_tree.pkl'):\n",
    "    with open('untrained_model_stats_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump(get_stats(untrained_model, get_art_dataset(check_ids_tree), torch.device('cuda'), best_threshold_untrained_tree, True, best_nms_untrained_tree), f)\n",
    "with open('untrained_model_stats_tree.pkl', 'rb') as f:\n",
    "    untrained_model_stats_tree = pickle.load(f)\n",
    "if not os.path.exists('untrained_model_stats_person.pkl'):\n",
    "    with open('untrained_model_stats_person.pkl', 'wb+') as f:\n",
    "        pickle.dump(get_stats(untrained_model, get_art_dataset(check_ids_person), torch.device('cuda'), best_threshold_untrained_person, False, best_nms_untrained_person), f)\n",
    "with open('untrained_model_stats_person.pkl', 'rb') as f:\n",
    "    untrained_model_stats_person = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(untrained_model_stats_tree[0], untrained_model_stats_tree[1], untrained_model_stats_tree[2], untrained_model_stats_tree[3], 'untrained_tree_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(untrained_model_stats_person[0], untrained_model_stats_person[1], untrained_model_stats_person[2], untrained_model_stats_person[3], 'untrained_person_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_augmentation_model = load_model('Without_Augmentations')\n",
    "no_augmentation_model.train()\n",
    "\n",
    "if not os.path.exists('best_nms_and_result_no_augmentation_tree.pkl'):\n",
    "    best_nms_no_augmentation_tree = None\n",
    "    best_result_no_augmentation_tree = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 21)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(no_augmentation_model, score_threshold=0, device=best_config['device'],\n",
    "                                                   for_tree=True, nms=t)\n",
    "        if current_score > best_result_no_augmentation_tree:\n",
    "            best_result_no_augmentation_tree = current_score\n",
    "            best_nms_no_augmentation_tree = t\n",
    "    with open('best_nms_and_result_no_augmentation_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_nms_no_augmentation_tree, best_result_no_augmentation_tree), f)\n",
    "else:\n",
    "    with open('best_nms_and_result_no_augmentation_tree.pkl', 'rb') as f:\n",
    "        best_nms_no_augmentation_tree, best_result_no_augmentation_tree = pickle.load(f)\n",
    "print(f'{best_nms_no_augmentation_tree=}, {best_result_no_augmentation_tree=}')\n",
    "\n",
    "if not os.path.exists('best_threshold_and_result_no_augmentation_tree.pkl'):\n",
    "    best_threshold_no_augmentation_tree = None\n",
    "    best_result_no_augmentation_tree = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 11)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(no_augmentation_model, score_threshold=t, device=best_config['device'],\n",
    "                                                   for_tree=True, nms=best_nms_no_augmentation_tree)\n",
    "        if current_score > best_result_no_augmentation_tree:\n",
    "            best_result_no_augmentation_tree = current_score\n",
    "            best_threshold_no_augmentation_tree = t\n",
    "    with open('best_threshold_and_result_no_augmentation_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_threshold_no_augmentation_tree, best_result_no_augmentation_tree), f)\n",
    "else:\n",
    "    with open('best_threshold_and_result_no_augmentation_tree.pkl', 'rb') as f:\n",
    "        best_threshold_no_augmentation_tree, best_result_no_augmentation_tree = pickle.load(f)\n",
    "print(f'{best_threshold_no_augmentation_tree=}, {best_result_no_augmentation_tree=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('best_nms_and_result_no_augmentation_person.pkl'):\n",
    "    best_nms_no_augmentation_person = None\n",
    "    best_result_no_augmentation_person = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 21)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(no_augmentation_model, score_threshold=0, device=best_config['device'],\n",
    "                                                   for_tree=False, nms=t)\n",
    "        if current_score > best_result_no_augmentation_person:\n",
    "            best_result_no_augmentation_person = current_score\n",
    "            best_nms_no_augmentation_person = t\n",
    "    with open('best_nms_and_result_no_augmentation_person.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_nms_no_augmentation_person, best_result_no_augmentation_person), f)\n",
    "else:\n",
    "    with open('best_nms_and_result_no_augmentation_person.pkl', 'rb') as f:\n",
    "        best_nms_no_augmentation_person, best_result_no_augmentation_person = pickle.load(f)\n",
    "print(f'{best_nms_no_augmentation_person=}, {best_result_no_augmentation_person=}')\n",
    "\n",
    "if not os.path.exists('best_threshold_and_result_no_augmentation_person.pkl'):\n",
    "    best_threshold_no_augmentation_person = None\n",
    "    best_result_no_augmentation_person = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 11)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset(no_augmentation_model, score_threshold=t, device=best_config['device'],\n",
    "                                                   for_tree=False, nms=best_nms_no_augmentation_person)\n",
    "        if current_score > best_result_no_augmentation_person:\n",
    "            best_result_no_augmentation_person = current_score\n",
    "            best_threshold_no_augmentation_person = t\n",
    "    with open('best_threshold_and_result_no_augmentation_person.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_threshold_no_augmentation_person, best_result_no_augmentation_person), f)\n",
    "else:\n",
    "    with open('best_threshold_and_result_no_augmentation_person.pkl', 'rb') as f:\n",
    "        best_threshold_no_augmentation_person, best_result_no_augmentation_person = pickle.load(f)\n",
    "print(f'{best_threshold_no_augmentation_person=}, {best_result_no_augmentation_person=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(best_model_stats_tree[0], best_model_stats_tree[1], best_model_stats_tree[2], best_model_stats_tree[3], 'best_tree_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(best_model_stats_person[0], best_model_stats_person[1], best_model_stats_person[2], best_model_stats_person[3], 'best_person_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('no_augmentation_model_stats_tree.pkl'):\n",
    "    with open('no_augmentation_model_stats_tree.pkl', 'wb+') as f:\n",
    "        pickle.dump(get_stats(no_augmentation_model, get_art_dataset(check_ids_tree), torch.device('cuda'), best_threshold_no_augmentation_tree, True, best_nms_no_augmentation_tree), f)\n",
    "with open('no_augmentation_model_stats_tree.pkl', 'rb') as f:\n",
    "    no_augmentation_model_stats_tree = pickle.load(f)\n",
    "if not os.path.exists('no_augmentation_model_stats_person.pkl'):\n",
    "    with open('no_augmentation_model_stats_person.pkl', 'wb+') as f:\n",
    "        pickle.dump(get_stats(no_augmentation_model, get_art_dataset(check_ids_person), torch.device('cuda'), best_threshold_no_augmentation_person, False, best_nms_no_augmentation_person), f)\n",
    "with open('no_augmentation_model_stats_person.pkl', 'rb') as f:\n",
    "    no_augmentation_model_stats_person = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(no_augmentation_model_stats_tree[0], no_augmentation_model_stats_tree[1], no_augmentation_model_stats_tree[2], no_augmentation_model_stats_tree[3], 'no_augmentation_tree_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(no_augmentation_model_stats_person[0], no_augmentation_model_stats_person[1], no_augmentation_model_stats_person[2], no_augmentation_model_stats_person[3], 'no_augmentation_person_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "del no_augmentation_model\n",
    "gc.collect()\n",
    "coco_model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "coco_model.to(torch.device('cuda'))\n",
    "\n",
    "def get_stats_coco(from_model, ds, device, score_threshold, nms=0.1):\n",
    "    check_ids = check_ids_person\n",
    "    from_model.to(device)\n",
    "    from_model.eval()\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    identifiers_false_positive = []\n",
    "    identifiers_false_negative = []\n",
    "    for ident in (loading_bar := tqdm(check_ids, total=len(check_ids))):\n",
    "        stats_img, _ = ds.get_item_by_identifier(ident)\n",
    "        stats_output = from_model([stats_img.to(device)])\n",
    "        stats_reduced = reduce_box_count(stats_output[0], nms, score_threshold, 0)\n",
    "        stats_labels = stats_reduced['labels']\n",
    "\n",
    "        del stats_img\n",
    "        del stats_output\n",
    "        del stats_reduced\n",
    "        gc.collect()\n",
    "\n",
    "        found = 1 in stats_labels\n",
    "        exists = da.get_person_annotation_for_identifier(ident)\n",
    "\n",
    "        if exists:\n",
    "            pos += 1\n",
    "            if found:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "                identifiers_false_negative.append(ident)\n",
    "        else:\n",
    "            neg += 1\n",
    "            if found:\n",
    "                false_positive += 1\n",
    "                identifiers_false_positive.append(ident)\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        loading_bar.set_description(\n",
    "            f'{true_positive=}, {false_positive=}, {true_negative=}, {false_negative=}, {pos=}, {neg=}')\n",
    "    return true_positive, false_positive, true_negative, false_negative, pos, neg, identifiers_false_positive, identifiers_false_negative\n",
    "\n",
    "def evaluate_model_for_dataset_coco(model_to_evaluate, score_threshold, device, nms):\n",
    "    check_ids = check_ids_person\n",
    "    stats = get_stats_coco(model_to_evaluate, get_art_dataset(check_ids), device, score_threshold, nms=nms)\n",
    "    true_positive = stats[0]\n",
    "    false_positive = stats[1]\n",
    "    false_negative = stats[3]\n",
    "    if (true_positive + false_positive) == 0 or (true_positive + false_negative) == 0:\n",
    "        return 0\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "if not os.path.exists('best_nms_and_result_coco_person.pkl'):\n",
    "    best_nms_coco_person = None\n",
    "    best_result_coco_person = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 21)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset_coco(coco_model, score_threshold=0, device=best_config['device'], nms=t)\n",
    "        if current_score > best_result_coco_person:\n",
    "            best_result_coco_person = current_score\n",
    "            best_nms_coco_person = t\n",
    "    with open('best_nms_and_result_coco_person.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_nms_coco_person, best_result_coco_person), f)\n",
    "else:\n",
    "    with open('best_nms_and_result_coco_person.pkl', 'rb') as f:\n",
    "        best_nms_coco_person, best_result_coco_person = pickle.load(f)\n",
    "print(f'{best_nms_coco_person=}, {best_result_coco_person=}')\n",
    "\n",
    "if not os.path.exists('best_threshold_and_result_coco_person.pkl'):\n",
    "    best_threshold_coco_person = None\n",
    "    best_result_coco_person = 0\n",
    "\n",
    "    for t in [0.05 * i for i in range(0, 11)]:\n",
    "        print(f'testing with: {t}')\n",
    "        current_score = evaluate_model_for_dataset_coco(coco_model, score_threshold=t, device=best_config['device'], nms=best_nms_coco_person)\n",
    "        if current_score > best_result_coco_person:\n",
    "            best_result_coco_person = current_score\n",
    "            best_threshold_coco_person = t\n",
    "    with open('best_threshold_and_result_coco_person.pkl', 'wb+') as f:\n",
    "        pickle.dump((best_threshold_coco_person, best_result_coco_person), f)\n",
    "else:\n",
    "    with open('best_threshold_and_result_coco_person.pkl', 'rb') as f:\n",
    "        best_threshold_coco_person, best_result_coco_person = pickle.load(f)\n",
    "print(f'{best_threshold_coco_person=}, {best_result_coco_person=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('coco_model_stats_person.pkl'):\n",
    "    with open('coco_model_stats_person.pkl', 'wb+') as f:\n",
    "        pickle.dump(get_stats_coco(coco_model, get_art_dataset(check_ids_person), torch.device('cuda'), best_threshold_coco_person, best_nms_coco_person), f)\n",
    "with open('coco_model_stats_person.pkl', 'rb') as f:\n",
    "    coco_model_stats_person = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_manual(coco_model_stats_person[0], coco_model_stats_person[1], coco_model_stats_person[2], coco_model_stats_person[3], 'coco_person_confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del coco_model\n",
    "gc.collect()\n",
    "\n",
    "unaugmented_results = dict()\n",
    "augmented_results = dict()\n",
    "\n",
    "def get_mcnemars_matrices():\n",
    "    model_name_1 = 'Without_Augmentations'\n",
    "    model_name_2 = 'With_Augmentations'\n",
    "\n",
    "    results = {'Tree': {model_name_1: dict(), model_name_2: dict()}, 'Person': {model_name_1: dict(), model_name_2: dict()}}\n",
    "\n",
    "    for model_name in [model_name_1, model_name_2]:\n",
    "        current_model = load_model(model_name)\n",
    "        current_model.to(torch.device('cuda'))\n",
    "        current_model.eval()\n",
    "        for for_tree in [True, False]:\n",
    "            if for_tree:\n",
    "                check_ids = check_ids_tree\n",
    "                current_dictionary = results['Tree']\n",
    "            else:\n",
    "                check_ids = check_ids_person\n",
    "                current_dictionary = results['Person']\n",
    "\n",
    "            mc_neamar_dataset = get_art_dataset(check_ids)\n",
    "\n",
    "            for ident in tqdm(check_ids, total=len(check_ids)):\n",
    "                mc_neamar_img, mc_neamar_target = mc_neamar_dataset.get_item_by_identifier(ident)\n",
    "                mc_neamar_output = current_model([mc_neamar_img.to(torch.device('cuda'))])\n",
    "\n",
    "                if model_name == 'With_Augmentations':\n",
    "                    if for_tree:\n",
    "                        ideal_nms = best_nms_tree\n",
    "                        ideal_threshold = best_threshold_tree\n",
    "                    else:\n",
    "                        ideal_nms = best_nms_person\n",
    "                        ideal_threshold = best_threshold_person\n",
    "                else:\n",
    "                    if for_tree:\n",
    "                        ideal_nms = best_nms_no_augmentation_tree\n",
    "                        ideal_threshold = best_threshold_no_augmentation_tree\n",
    "                    else:\n",
    "                        ideal_nms = best_nms_no_augmentation_person\n",
    "                        ideal_threshold = best_threshold_no_augmentation_person\n",
    "                mc_neamar_reduced = reduce_box_count(mc_neamar_output[0], ideal_nms, ideal_threshold, 0)\n",
    "                mc_neamar_labels = [da.get_class_label_for_index(l) for l in mc_neamar_reduced['labels']]\n",
    "\n",
    "                if for_tree:\n",
    "                    found = 'Baum' in mc_neamar_labels\n",
    "                    exists = da.get_tree_annotation_for_identifier(ident)\n",
    "                else:\n",
    "                    found = 'Person' in mc_neamar_labels\n",
    "                    exists = da.get_person_annotation_for_identifier(ident)\n",
    "\n",
    "                if exists:\n",
    "                    if found:\n",
    "                        current_dictionary[model_name][ident] = 'TP'\n",
    "                    else:\n",
    "                        current_dictionary[model_name][ident] = 'FN'\n",
    "                else:\n",
    "                    if found:\n",
    "                        current_dictionary[model_name][ident] = 'FP'\n",
    "                    else:\n",
    "                        current_dictionary[model_name][ident] = 'TN'\n",
    "        del current_model\n",
    "        gc.collect()\n",
    "\n",
    "    matrices = {'Tree': [[0,0], [0,0]], 'Person': [[0,0], [0,0]]}\n",
    "    correct_results = ['TP', 'TN']\n",
    "\n",
    "    for test in ['Tree', 'Person']:\n",
    "        if test == 'Tree':\n",
    "            check_ids = check_ids_tree\n",
    "        else:\n",
    "            check_ids = check_ids_person\n",
    "\n",
    "        for ident in check_ids:\n",
    "            result_1 = results[test][model_name_1][ident]\n",
    "            result_2 = results[test][model_name_2][ident]\n",
    "            if result_1 in correct_results and result_2 in correct_results:\n",
    "                matrices[test][0][0] += 1\n",
    "            elif result_1 in correct_results and result_2 not in correct_results:\n",
    "                matrices[test][0][1] += 1\n",
    "            elif result_1 not in correct_results and result_2 in correct_results:\n",
    "                matrices[test][1][0] += 1\n",
    "            elif result_1 not in correct_results and result_2 not in correct_results:\n",
    "                matrices[test][1][1] += 1\n",
    "            else:\n",
    "                raise Exception('Unhandled Case')\n",
    "\n",
    "    matrices['Both'] = [[matrices['Tree'][0][0] + matrices['Person'][0][0], matrices['Tree'][0][1] + matrices['Person'][0][1]], [matrices['Tree'][1][0] + matrices['Person'][1][0], matrices['Tree'][1][1] + matrices['Person'][1][1]]]\n",
    "\n",
    "    return matrices\n",
    "\n",
    "if os.path.exists(path := 'mcnemar_matrices.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        mcnemar_matrices = pickle.load(f)\n",
    "else:\n",
    "    mcnemar_matrices = get_mcnemars_matrices()\n",
    "    with open(path, 'wb+') as f:\n",
    "        pickle.dump(mcnemar_matrices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(mcnemar(mcnemar_matrices['Person'], exact=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The null hypothesis which claims that the two model's performances are equal can be rejected (p value < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(mcnemar(mcnemar_matrices['Tree'], exact=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Masters-Thesis-Implementation)",
   "language": "python",
   "name": "pycharm-8f91eb99"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
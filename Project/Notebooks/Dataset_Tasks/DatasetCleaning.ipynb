{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Set Cleaning\n",
    "In this Notebok I will discard information that is not needed to perform our tasks and I also bring the data into a format that can be worked with.\n",
    "\n",
    "Note: Some of the processes in this notebook may be somewhat inefficient or unoptimized, but since this notebook only runs once, that is alright.\n",
    "\n",
    "Note: Because of the license of the used dataset, it might be the case that the result of this notebook must be useable by other parties. The section \"Disclaimer\" in the READ.me file applies, as correctness cannot be guaranteed - especially with the data enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Project.Utils.Misc.Nlp import NLP\n",
    "from Project.Utils.TextTagProcessing.GermanWordSplitter import GermanWordSplitter\n",
    "from Project.Utils.IconclassCache.IconclassCache import IconclassCacheGenerationFailedException, NumberOfTriesExceededException, IconclassCache\n",
    "from Project.Utils.ConfigurationUtils.ConfigLoader import ConfigLoader\n",
    "from Project.VisualFeaturesBranches.ObjectDetection.ObjectDetectionNetworkUtils import get_results_path, train_model_on_all_data, get_winning_config_with_augmentations, get_winning_config_without_augmentations\n",
    "\n",
    "nlp = NLP.instance.nlp\n",
    "gws = GermanWordSplitter()\n",
    "cl: ConfigLoader = ConfigLoader.instance\n",
    "\n",
    "cl.data_cleaning_in_progress = True\n",
    "\n",
    "database_file = os.path.join(os.getcwd(), 'xmlkultur.xlsx')\n",
    "assert os.path.exists(database_file), 'The Dataset is not in the expected directory'\n",
    "\n",
    "image_dir = os.path.join(os.getcwd(), 'images')\n",
    "assert os.path.exists(image_dir), 'Image directory could not be found.'\n",
    "df = pd.read_excel(database_file, index_col=0)\n",
    "\n",
    "# Checking the uniqueness of the identifier\n",
    "assert len(df['Identifier'].unique()) == df.shape[0], 'Object Id is not a unique identifier'\n",
    "\n",
    "if not os.path.exists('DatasetCleaningCheckpoints'):\n",
    "    os.mkdir('DatasetCleaningCheckpoints')\n",
    "\n",
    "pickle_path_1 = os.path.join('DatasetCleaningCheckpoints', 'pickle_1.pkl')\n",
    "pickle_path_2 = os.path.join('DatasetCleaningCheckpoints', 'pickle_2.pkl')\n",
    "pickle_path_3 = os.path.join('DatasetCleaningCheckpoints', 'pickle_3.pkl')\n",
    "pickle_path_4 = os.path.join('DatasetCleaningCheckpoints', 'pickle_4.pkl')\n",
    "pickle_path_5 = os.path.join('DatasetCleaningCheckpoints', 'pickle_5.pkl')\n",
    "pickle_path_6 = os.path.join('DatasetCleaningCheckpoints', 'pickle_6.pkl')\n",
    "pickle_path_7 = os.path.join('DatasetCleaningCheckpoints', 'pickle_7.pkl')\n",
    "pickle_path_8 = os.path.join('DatasetCleaningCheckpoints', 'pickle_8.pkl')\n",
    "pickle_path_9 = os.path.join('DatasetCleaningCheckpoints', 'pickle_9.pkl')\n",
    "pickle_path_10 = os.path.join('DatasetCleaningCheckpoints', 'pickle_10.pkl')\n",
    "pickle_path_11 = os.path.join('DatasetCleaningCheckpoints', 'pickle_11.pkl')\n",
    "pickle_path_12 = os.path.join('DatasetCleaningCheckpoints', 'pickle_12.pkl')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dropping columns that will not be of much value further on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to educate our selection, I first check columns for NaNs and look for single valued columns, which would add no information at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple(f'{name}: {len(df[name].unique())}' for name in df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following columns are dropped right away:\n",
    "* 'IsShownAt'\n",
    "    * Unreleated to the task\n",
    "* 'ObjectType'\n",
    "    * Single valued column\n",
    "* 'Object'\n",
    "    * No longer needed\n",
    "* 'Format'\n",
    "    * No longer needed\n",
    "* 'Dimensions'\n",
    "    * Unreleated to the task\n",
    "* 'Location'\n",
    "    * Unreleated to the task\n",
    "* 'Rights'\n",
    "    * Single valued column\n",
    "* 'CreativeCommons'\n",
    "    * Single valued column\n",
    "* 'Publisher'\n",
    "    * Single valued column\n",
    "* 'Spatial'\n",
    "    * Unrelated to the task\n",
    "* 'Provenance'\n",
    "    * Unrelated to the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    columns=['IsShownAt', 'ObjectType', 'Object', 'Format', 'Dimensions', 'Location', 'Rights', 'CreativeCommons',\n",
    "             'Publisher', 'Spatial', 'Provenance'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I check again how many values exist per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[f'{name}: {len(df[name].unique())}' for name in df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[[entry for entry in df['CreationDate'] if not entry.isdigit()][i] for i in\n",
    " [0, 1, 9, 11, 13, 19, 82, 260, 290, 413, 578, 643, 877, 904]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This column will be very hard to clean, but it is necessary in order to put the images into a temporal order and to use them later on for self-supervised learning. Particularly the \"undatiert\" (undated) fields will be hard to substiitute. Luckily I have another source of information when it comes to the dates of the creations: The \"Temporal\" Row will be able to give us at least rough estimates of when a particular work was created. Unfortunately there will still be 55 images left that lack both pieces of information. I will explain how these are dealt with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df['Temporal'].unique())\n",
    "print(len(df[df['Temporal'].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Above we see the different epochs. There are 134 entries in the dataset which lack the association with a particular epoch.\n",
    "\n",
    "There is also a typo in 'BIedermeier-Realismus' where the second letter is written in capital. This will be corrected in the next cell, effectively merging the uncorrectly written category with the correctly written one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['Temporal'] == 'BIedermeier-Realismus', 'Temporal'] = 'Biedermeier-Realismus'\n",
    "print(df['Temporal'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the Data Enhancement of the \"Temporal\" column, Wikipedia articles were checked to find an approxiamte year. Please be aware that this labeling is a very rough approximation used only on a tiny part of the dataset (<100 of instances) and therefore not very scientific. An art historian will almost certainly disagree with the dates that were picked and said historian might also find some faults in the labeling altogether. It is also appearent that a single label is absent the information of how great the timespan of an epoche is. This can mean quite a large margin of error for longer epochs. However these labels are only needed to bring some ordering into the labels that lack a creation date, but have an epoch associated to them and for that task this rough estimate should certainly be sufficient. A perhaps more accurate way of labeling would be to look up each creator individually, which I will actually do for some of them. This problem with the large margin of error only concerns long epoch like the Renaissance or the \"Romanik\" and \"Barock\". Most creators of these epochs are unknown anyway. For Barock, however there exist a lot of rows in the database, but only a handful of creators. Therefore I will use the lifetime of the creators to create the labels in these cases. There is also one known creator in \"Romanik\", which I also looked up.\n",
    "\n",
    "In the following there is a list with short description of why certain dates were chosen. These are necessary in order for a reader to determine wheter a mistake (like a typo or miscalculation) was made in the process of the labeling.\n",
    "\n",
    "* \"Kunst um 1900\"\n",
    "    * Will obviously be labeled with 1900\n",
    "* \"Moderne vor 1945\"\n",
    "    * \"https://de.wikipedia.org/wiki/Moderne\" states that the period started in literature- and arthistory with the beginning of the 19th century, and as style with the end of the 19th century. I therefore take the end of the 19th century, as the style is something that will be appearent in the works of art. However, the label says \"Moderne vor 1945\" so I roughly average the year to 1925, so that it is later then \"Kunst um 1900\".\n",
    "* \"Realismus\"\n",
    "    * https://de.wikipedia.org/wiki/Realismus_(Kunst) states that the artworks of this epoch have their beginnings in mid 19th century in Europe. As I understand it, the epoch ended with the early 20th century, so the label chosen is 1860.\n",
    "* \"Historismus\"\n",
    "    * https://de.wikipedia.org/wiki/Historismus states that this epoch dates to the late 19th and early 20th century, therefore  1900 is chosen again.\n",
    "* \"Spätbarrock\"\n",
    "    * https://de.wikipedia.org/wiki/Barock states the years 1700-1730 for \"Spätbarrock\", but also states that sometimes people use the name also when talking about the epoch \"Rokoko\" which goes from 1730 to 1760/70. Since there is no epoch called \"Rokoko\" in our dataset, the assumption is made, that this is also the case in the dataset at hand. Therefore the year 1730 is chosen.\n",
    "* \"Romantik\"\n",
    "    * https://de.wikipedia.org/wiki/Romantik states that the epoch started at the ending of the 18th century and lasted until the far into the 19th century. Therefore the label 1840 is chosen.\n",
    "* \"Bidermeier-Realismus\"\n",
    "    * The difference between Bidermeier and Bidermeier-Realismus was not appearent for me, which is why they share the same date (see further below in this list)\n",
    "* \"Neoklassizismus\"\n",
    "    * https://de.wikipedia.org/wiki/Neoklassizismus_(bildende_Kunst) states the beginning around 1900, but is not so clear about the end of the epoch. As I understand it, this seems to be in the 40s or 50s, which is why the date 1920 is chosen as label.\n",
    "* \"Symbolismus\"\n",
    "    * https://de.wikipedia.org/wiki/Symbolismus_(bildende_Kunst) puts the height of the epoch between 1880 and 1910. Therefore the date 1895 is chosen.\n",
    "* \"Klassizismus\"\n",
    "    * https://de.wikipedia.org/wiki/Klassizismus puts the time of this epoch to approximately 1770 and 1840. The label will therefore be 1805.\n",
    "* \"Barock\"\n",
    "    * https://de.wikipedia.org/wiki/Barock puts the epoch in the timeframe between the end of the 16th century and to epproximately 1760/70, therefore the label 1680 is used.\n",
    "* \"Stimmungsimpressionismus\"\n",
    "    * https://de.wikipedia.org/wiki/Stimmungsimpressionismus puts this epoch in the years from 1870 to 1900, therefore the label will be 1885.\n",
    "* \"Spätgotik\"\n",
    "    * https://de.wikipedia.org/wiki/Malerei_in_der_Gotik puts the end of this epoch to 1525/30 and https://de.wikipedia.org/wiki/Gotik#Regionale_Verbreitung_und_Weiterentwicklung puts the beginning of it to 1350 (although this year was stated concerning the architecture of buildings!). The label chosen is 1450.\n",
    "* \"Biedermeier\"\n",
    "    * https://de.wikipedia.org/wiki/Biedermeier dates the epoch to 1815-1848, therefore the roughly averaged label is 1835.\n",
    "* \"Moderne nach 1945\"\n",
    "    * https://de.wikipedia.org/wiki/Moderne discusses a \"Zweite Moderne\" (second \"Moderne\"), which is, as I understand still ongoing, which is why the label 1980 is chosen.\n",
    "* \"Impressionismus\"\n",
    "    * https://de.wikipedia.org/wiki/Impressionismus_(Malerei) puts the beginning of this epoch into the second half of the 19th century and the epoch seems to go to the beginning of the next century. Therefore the date 1880 is chosen, which puts it a bit earlier than the label of \"Stimmungsimpressionismus\".\n",
    "* \"Neue Sachlichkeit\"\n",
    "    * https://de.wikipedia.org/wiki/Neue_Sachlichkeit_(Kunst) states that the year 1925 is very important for this epoch and it seems to be right in the middle of its timespan as well, which is why this year was chosen as label.\n",
    "* \"Realismus; Naturalismus\"\n",
    "    * The average of the labels I put on \"Realismus\" (see further up in the list) and \"Naturalismus\" (see further down in the list) is 1865, so this year will be used.\n",
    "* \"Expressionismus\"\n",
    "    * https://de.wikipedia.org/wiki/Expressionismus puts this epoch into the early 20th century, so the year 1915 is chosen.\n",
    "* \"Hochbarock\"\n",
    "    * https://de.wikipedia.org/wiki/Barock puts this epoch to approximately in 1850 to 1900, which is why it is labeled 1875.\n",
    "* \"Spätimpressionismus\"\n",
    "    * https://de.wikipedia.org/wiki/Post-Impressionismus states that \"Spätimpressionismus\" and \"Postimpressionismus\" are synonyms which is why I put the same label on it as in \"Postimpressionismus\" (see further below in the list).\n",
    "* \"Renaissance\"\n",
    "    * https://de.wikipedia.org/wiki/Renaissance puts the most important time of this epoch to the 15th and 16th century, which is why I choose the label 1500\n",
    "* \"Klassische Moderne\"\n",
    "    * https://de.wikipedia.org/wiki/Moderne has a quote from \"Detlev Peukert: Max Webers Diagnose der Moderne. Vandenhoeck & Ruprecht, Göttingen 1989, ISBN 3-525-33562-8\" where he states that the time of the Weirmarer Republic was the height of the \"Klassische Moderne\", which is why in our labeling, the year 1925 was chosen (which is the same label which was put on \"Moderne vor 1945\").\n",
    "* \"Naturalismus\"\n",
    "    * https://de.wikipedia.org/wiki/Naturalismus_(bildende_Kunst) puts this epoch from approximately 1870 to 1890, therefore the label will be 1880.\n",
    "* \"Postimpressionismus\"\n",
    "    * https://de.wikipedia.org/wiki/Post-Impressionismus puts this epoch between 1880 and 1905, which is why it gets the label 1890.\n",
    "* \"Frühbarock\"\n",
    "    * https://de.wikipedia.org/wiki/Barock puts the epoch to approximately 1650.\n",
    "* \"Neuromantik\"\n",
    "    * https://de.wikipedia.org/wiki/Neuromantik puts this epoch in 1890 to 1915 (altough it refers to the literary epoch). I therefore label it with 1905.\n",
    "* \"Hochrenaissance\"\n",
    "    * https://de.wikipedia.org/wiki/Hochrenaissance puts the epoch in approximately 1500 to 1530, which is why 1515 was chosen as a label.\n",
    "* \"Klassizismus; Biedermeier-Realismus\"\n",
    "    * Since \"Klassizismus\" is labeled with 1805 and \"Biedermeier-Realismus\" with 1835, the chosen level is the average of 1820.\n",
    "* \"Spätromantik\"\n",
    "    * https://de.wikipedia.org/wiki/Sp%C3%A4tromantik puts this epoch in the time between 1815 und 1848, which is why 1830 is chosen as the label.\n",
    "* \"Romanik\"\n",
    "    * https://de.wikipedia.org/wiki/Romanik states that the beginning of this eoch is around 950/60 and the end depends on the country, but is in the 13 century at the latest. It should be mentioned that here again, the years in the article refer to the architecture. 1100 is chosen as label.\n",
    "* \"Historismus; Naturalismus\"\n",
    "    * Since the label for \"Historismus\" is 1900 and the label for \"Naturalismus\" is 1880, the average is chosen as new label, which is 1890.\n",
    "* \"Realismus; Symbolismus\"\n",
    "    * Since the label for \"Realismus\" is 1850 and the label for \"Symbolismus\" is 1895, the rough average is chosen as new label, which is 1875.\n",
    "* \"Frühgotik\"\n",
    "    * https://de.wikipedia.org/wiki/Gotik puts this epoch between 1130 and 1180 which is why the label 1155 is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Artists in these categories are unknown as discussed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[df['Temporal'] == 'Renaissance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[df['Temporal'] == 'Romanik']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "7 creators from the epooch of \"Barrock\" are known, as can be seen in the cell below. I will look them up together with the other creators further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(df[df['Temporal'] == 'Barock']))\n",
    "print(df[df['Temporal'] == 'Barock']['Creator'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temporal_to_year = {\n",
    "    'Kunst um 1900': 1900,\n",
    "    'Moderne vor 1945': 1925,\n",
    "    'Realismus': 1860,\n",
    "    'Historismus': 1900,\n",
    "    'Spätbarock': 1730,\n",
    "    'Romantik': 1840,\n",
    "    'Biedermeier-Realismus': 1835,\n",
    "    'Neoklassizismus': 1920,\n",
    "    'Symbolismus': 1895,\n",
    "    'Klassizismus': 1805,\n",
    "    'Barock': 1680,\n",
    "    'Stimmungsimpressionismus': 1885,\n",
    "    'Spätgotik': 1450,\n",
    "    'Biedermeier': 1835,\n",
    "    'Moderne nach 1945': 1980,\n",
    "    'Impressionismus': 1880,\n",
    "    'Neue Sachlichkeit': 1925,\n",
    "    'Realismus; Naturalismus': 1865,\n",
    "    'Expressionismus': 1915,\n",
    "    'Hochbarock': 1875,\n",
    "    'Spätimpressionismus': 1890,\n",
    "    'Renaissance': 1500,\n",
    "    'Klassische Moderne': 1925,\n",
    "    'Naturalismus': 1880,\n",
    "    'Postimpressionismus': 1890,\n",
    "    'Frühbarock': 1650,\n",
    "    'Neuromantik': 1905,\n",
    "    'Hochrenaissance': 1515,\n",
    "    'Klassizismus; Biedermeier-Realismus': 1820,\n",
    "    'Spätromantik': 1830,\n",
    "    'Romanik': 1100,\n",
    "    'Historismus; Naturalismus': 1890,\n",
    "    'Realismus; Symbolismus': 1875,\n",
    "    'Frühgotik': 1155,\n",
    "    np.nan: -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are rows which have no entry in \"CreationDate\". For these two ways to handle them are appearent:\n",
    "* Option 1: There are 20 unique creators for the creators in question and I could label the images with the average of their lifetime. For the entries where also the artist is not known I would then use option 2:\n",
    "* Option 2: Look up what collections these images where part of and average their epoch-labels (which I have already created)\n",
    "\n",
    "Option 1 has the advantage that it would be fairly close to the real date, for that reason I will go with that wherever possible. Since I am looking at hundrets of years of art (and my knowledge of art is very limited at this point), I am susceptible to confusing artists with the same name. Therefore only sammlung.belvedere.at is used as a source and if the information cannot be gathered there, option 2 is used. The perhaps greater argument behind this is, that if the art experts that labeled these images could not find a date, then the date that is found with one quick google search is probably not the right one.\n",
    "\n",
    "All resources in this list where accessed on 15.08.2021.\n",
    "\n",
    "The years between birth and death will be roughly averaged for the labeling.\n",
    "\n",
    "* E. Guenther\n",
    "    * https://sammlung.belvedere.at/people/679/e-guenther -> 19th century -> Therefore we take 1850 as a date\n",
    "* Anton Hans Karlinsky\n",
    "    * https://sammlung.belvedere.at/people/1003/anton-hans-karlinsky/ -> 1872 - 1945\n",
    "* Franz Schmied\n",
    "    * https://sammlung.belvedere.at/people/objects/2028 -> 1796 - 1851\n",
    "* Joseph Hasslwander\n",
    "    * https://sammlung.belvedere.at/people/747/joseph-hasslwander/ -> 1812 - 1878\n",
    "* Franz Hoffmann\n",
    "    * https://sammlung.belvedere.at/people/8985/franz-hoffmann/ -> Nothing, therefore option 2 will be used\n",
    "* P. Dupin\n",
    "    * https://sammlung.belvedere.at/people/6320/pierre-dupin -> Nothing\n",
    "* Joseph Lavos\n",
    "    * https://sammlung.belvedere.at/people/1243/joseph-lavos -> 1807 - 1848\n",
    "* Eduard Gurk\n",
    "    * https://sammlung.belvedere.at/people/687/eduard-gurk -> 1801 - 1841\n",
    "* Carl Heindel\n",
    "    * https://sammlung.belvedere.at/people/774/carl-heindel -> 1810 - 1869\n",
    "* Viktor Scharf\n",
    "    * https://sammlung.belvedere.at/people/1980/viktor-scharf -> 1872 - 1943 (?)\n",
    "* Friedrich Hasslwander\n",
    "    * https://sammlung.belvedere.at/people/746/friedrich-hasslwander/objects? -> 1840 - 1914\n",
    "* Leopold Pollak\n",
    "    * https://sammlung.belvedere.at/people/1736/leopold-pollak -> 1806 - 1880\n",
    "* Balthasar Moncornet\n",
    "    * https://sammlung.belvedere.at/people/6318/balthasar-moncornet -> approximately 1600 to 1668\n",
    "* Bernhard Reinhold\n",
    "    * https://sammlung.belvedere.at/people/1832/bernhard-reinhold -> 1824 - 1892\n",
    "* Peter Schenk der Ältere\n",
    "    * https://sammlung.belvedere.at/people/7980/peter-schenk-der-altere -> 1660 - 1730\n",
    "* Francesco Milani\n",
    "    * https://sammlung.belvedere.at/people/1449/francesco-milani -> Nothing\n",
    "* Erasmus von Engert\n",
    "    * https://sammlung.belvedere.at/people/434/erasmus-von-engert/objects? -> 1796 - 1871\n",
    "* Lore Scheid\n",
    "    * https://sammlung.belvedere.at/people/1990/lore-scheid/objects? -> 1884 - 1946\n",
    "* Felice Zuliani\n",
    "    * https://sammlung.belvedere.at/people/2647/felice-zuliani/objects? -> (?) - 1834 -> The label will be 1810\n",
    "* Johann Peter Krafft\n",
    "    * https://sammlung.belvedere.at/people/1136/johann-peter-krafft/objects? -> 1780 - 1856\n",
    "\n",
    "As mentioned above, I also want to look up the artists from the period of \"Barock\" since the significant length of the epoch would mean that the average error would be rather high.\n",
    "\n",
    "* Salomon Kleiner\n",
    "    * https://sammlung.belvedere.at/people/1055/salomon-kleiner -> 1700 - 1761\n",
    "* Jakob Kellner\n",
    "    * https://sammlung.belvedere.at/people/6545/jakob-kellner -> ? - 1775 -> The label will be 1750\n",
    "* Simon Hueter\n",
    "    * https://sammlung.belvedere.at/people/888/simon-hueter -> Nothing\n",
    "* Johann Baptist Gumpp\n",
    "    * https://sammlung.belvedere.at/people/8065/johann-baptist-gumpp -> 1651 -1728\n",
    "* Jeremias Jakob Sedelmayr\n",
    "    * https://sammlung.belvedere.at/people/7764/jeremias-jakob-sedelmayr -> 1706 - 1761\n",
    "* Martin van Meytens d. J.\n",
    "    * https://sammlung.belvedere.at/people/1435/martin-van-meytens-d-j/objects? -> 1695-1770\n",
    "* Martino Altomonte\n",
    "    * https://sammlung.belvedere.at/people/32/martino-altomonte -> 1657/1659 – 1745\n",
    "\n",
    "For the one artist in Romanik that is actually known I have also looked up the date:\n",
    "* Meister I.P.\n",
    "    * https://sammlung.belvedere.at/people/5763/meister-i-p -> Mentions that the artist created art around 1520 / 1540 -> 1530 is used therefore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "creator_to_year = {\n",
    "    'E. Guenther': 1850,\n",
    "    'Anton Hans Karlinsky': 1910,\n",
    "    'Franz Schmied': 1825,\n",
    "    'Joseph Hasslwander': 1845,\n",
    "    'Franz Hoffmann': -1,\n",
    "    'P. Dupin': -1,\n",
    "    'Joseph Lavos': 1830,\n",
    "    'Eduard Gurk': 1821,\n",
    "    'Carl Heindel': 1840,\n",
    "    'Viktor Scharf': 1907,\n",
    "    'Friedrich Hasslwander': 1877,\n",
    "    'Leopold Pollak': 1843,\n",
    "    'Balthasar Moncornet': 1634,\n",
    "    'Bernhard Reinhold': 1858,\n",
    "    'Peter Schenk der Ältere': 1695,\n",
    "    'Francesco Milani': -1,\n",
    "    'Erasmus von Engert': 1835,\n",
    "    'Lore Scheid': 1915,\n",
    "    'Felice Zuliani': 1810,\n",
    "    'Johann Peter Krafft': 1818,\n",
    "    # Artists from the epoch of \"Barock\":\n",
    "    'Salomon Kleiner': 1730,\n",
    "    'Jakob Kellner': 1750,\n",
    "    'Simon Hueter': -1,\n",
    "    'Johann Baptist Gumpp': 1690,\n",
    "    'Jeremias Jakob Sedelmayr': 1733,\n",
    "    'Martin van Meytens d. J.': 1730,\n",
    "    'Martino Altomonte': 1700,\n",
    "    # Artists from the epoch of \"Romanik\":\n",
    "    'Meister I.P.': 1530,\n",
    "    # Unknown artists:\n",
    "    'Unbekannter Künstler': -1,\n",
    "    'Unbekannter Stecher': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below the few examples that will still lack a label can be seen. Interestingly they all come from the same 2 collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[(df['CreationDate'] == 'undatiert') & (df['Temporal'].map(temporal_to_year) == -1) & (\n",
    "        df['Creator'].map(creator_to_year) == -1)][['CreationDate', 'Temporal', 'Creator', 'Collection']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that all the columns that are still left are either part of \"Klassizismus | Romantik | Biedermeier ## Classicism | Romanticism | Biedermeier\" or \"Barrock ## Baroque\". I have previously labeld \"Romantik\" with the year 1840, \"Biedermeier\" with the year 1835 and \"Klassizismus; Biedermeier-Realismus\" with 1820. For that reason I will put the label 1830 on the objects that are still not labeled and that are part of the first collection. For the still unlabeled objects from the second collection I will use the label I assigned to \"Barock\" previously, which is 1680."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First however I correct two typos that were found in this column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['CreationDate'] == '1902 um', 'CreationDate'] = 'um 1902'\n",
    "df.loc[df['CreationDate'] == 'um 1800 /1820', 'CreationDate'] = 'um 1800/1820'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With all this out of the way, I am ready for creating our new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[df['Creator'] == 'Adolf Hirémy-Hirschl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['YearEstimate'] = ''\n",
    "to_update = dict()\n",
    "\n",
    "for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    current = row['CreationDate']\n",
    "    result = 0\n",
    "\n",
    "    # Treating special cases first\n",
    "    if current == \"undatiert\":\n",
    "\n",
    "        # For \"Barock\" artists I use the date information from the artists as discussed above\n",
    "        # Note that I only want to do this if we can actually associate the creator to a year\n",
    "        if row['Temporal'] == \"Barock\" and creator_to_year[row['Creator']] != -1:\n",
    "            result = creator_to_year[row['Creator']]\n",
    "        \n",
    "        # The same goes for the epoch of \"Romanik\"\n",
    "        elif row['Temporal'] == \"Romanik\" and creator_to_year[row['Creator']] != -1:\n",
    "            result = creator_to_year[row['Creator']]\n",
    "        \n",
    "        # Most of the images without a date can be matched with an epoch\n",
    "        elif (temporal_year := temporal_to_year[row['Temporal']]) != -1:\n",
    "            result = temporal_year\n",
    "\n",
    "        # For some the authors had to be looked up\n",
    "        elif (creator_year := creator_to_year[row['Creator']]) != -1:\n",
    "            result = creator_year\n",
    "\n",
    "        # For a few the labels have to be based solely on what collection they were part of\n",
    "        elif row['Collection'] == 'Klassizismus | Romantik | Biedermeier ## Classicism | Romanticism | Biedermeier':\n",
    "            result = 1830\n",
    "        elif row['Collection'] == 'Barock ## Baroque':\n",
    "            result = 1680\n",
    "        else:\n",
    "            raise Exception(f'Row is incorrectly handled\\n{row}')\n",
    "\n",
    "    # Now I take care of 7 particular entries that are written in an uncommon manner, so that breaking them\n",
    "    # down programmatically would not make a lot of sense.\n",
    "    elif current == '1778 oder 1788':\n",
    "        result = 1783\n",
    "    elif current == 'Ende 12. Jahrhundert / um 1200':\n",
    "        result = 1200\n",
    "    elif current == 'Ende 1878 oder 1880':\n",
    "        result = 1879\n",
    "    elif current == '1878 oder 1880':\n",
    "        result = 1879\n",
    "    # If this means 19th century then it is possible that this is mislabeled since the 19th century would span the time\n",
    "    # between 1800 and 1900.\n",
    "    elif current == 'Ende 19.–Anfang 2000':\n",
    "        result = 2000\n",
    "    # Same situation here\n",
    "    elif current == '19.–Anfang 2000':\n",
    "        result = 1975\n",
    "    elif current == 'Ende 19.–Anfang 20. Jahrhundert':\n",
    "        result = 1900\n",
    "\n",
    "    # Now to the \"general\" cases\n",
    "    else:\n",
    "\n",
    "        # Some of the list elements appear only once, but the differents to the special cases (further below) is that\n",
    "        # they do not influence the result variable, because they are not keywords like \"Anfang\" (meaning \"beginning\").\n",
    "        # Therefore they can be truncated\n",
    "\n",
    "        # Note: The order is important: Combinations like \"wohl um \" are also removed\n",
    "        leading = ['wohl ', 'um ', 'vor ', 'nach ', 'spätestens ', 'dem Sommer ']\n",
    "\n",
    "        trailing = [' ?', ' (?)', 'er Jahre', ' (Druck: 1798)', ' (Druck: 1796)', ' oder bald danach',\n",
    "                    ' (Guss: 1929/1930)', ' um', ' (nach 1483?)', ' (Guss: 1943)', ' (vollendet 1909)',\n",
    "                    ' (geringfügige Ergänzungen 1907)', ' (Guss 1927)', ' (Nachguss: 1980)',\n",
    "                    ' (1931 signiert)', ', 1924 überarbeitet']\n",
    "\n",
    "        for l in leading:\n",
    "            if current.startswith(l):\n",
    "                current = current[len(l):]\n",
    "\n",
    "        for t in trailing:\n",
    "            if current.endswith(t):\n",
    "                current = current[:-len(t)]\n",
    "\n",
    "        # e.g. 19. Jahrhundert -> 1800\n",
    "        jahrhundert_correction = 0\n",
    "        if current.endswith('. Jahrhundert'):\n",
    "            current = current[0:-13] + \"00\"\n",
    "            jahrhundert_correction = 100\n",
    "        elif current.endswith('. Jahrhunderts'):\n",
    "            current = current[0:-14] + \"00\"\n",
    "            jahrhundert_correction = 100\n",
    "\n",
    "        # Now I add a certain amount of years based on the keywords that precede the year\n",
    "        leading_keywords = {\n",
    "            'Anfang ': 10,\n",
    "            'Ende der ': 90,\n",
    "            'Ende ': 90,\n",
    "            '2. Hälfte des ': 75,\n",
    "            '4. Viertel ': 90,\n",
    "            '2. Viertel ': 40,\n",
    "            'Letztes Drittel ': 80\n",
    "        }\n",
    "\n",
    "        for key, value in leading_keywords.items():\n",
    "            if current.startswith(key):\n",
    "                current = current[len(key):]\n",
    "                result = value\n",
    "                break\n",
    "\n",
    "        # e.g. 1900\n",
    "        if len(current) == 4 and current.isdigit():\n",
    "            result += int(current)\n",
    "            result -= jahrhundert_correction\n",
    "        # e.g. 1900-1901 or 1900/1901\n",
    "        elif len(current) == 9 and current[0:4].isdigit() and current[6:].isdigit():\n",
    "            assert jahrhundert_correction == 0\n",
    "            result = int((int(current[0:4]) + int(current[5:])) / 2)\n",
    "        # e.g. 1900/01\n",
    "        elif len(current) == 7 and current[0:4].isdigit() and current[6:].isdigit():\n",
    "            assert jahrhundert_correction == 0\n",
    "            result = int(int(current[0:2]) * 100 + int(int(current[2:4]) + int(current[5:])) / 2)\n",
    "        else:\n",
    "            raise Exception(f'Row not taken care of: \\n{row}')\n",
    "\n",
    "    df.loc[ind, 'YearEstimate'] = result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are now no undefined values left in the coulmn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(df[(df['YearEstimate'].isin([-1, '', 0])) | (df['YearEstimate'].isna())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I also check if the Barock artists are labeled correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[df['Temporal'] == 'Barock'].groupby('Creator').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are still a few columns left, where its not clear whether they shuld be in the dataset or not. I will take a closer look at them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df['ObjectClass'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The \"ObjectClass\" column could prove to be interesting in further processing. If I consider for example that a \"Landkarte\" (map) would likely fill the whole space of the picture, while a 'Skulptur' (sculpture) is likely placed in front of a background, it could mean that differentiating between such categories could be beneficial e.g. for self supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df['MaterialTechnique'].unique()[0:50])\n",
    "print(len(df['MaterialTechnique'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Although the \"MaterialTechnique\" column has the power to indicate visual similarities, the small excerpt from the dataset shows already that the cleaning of this column would be hours of work. Furthermore, the aim of the project is not so much finding visually similar objects, but rather contextually similar ones, which means that focusing too much on the former would not help a lot in reaching the goal. Therefore this column should be dropped. However, there is a chance that the distribution of the values allows that the number of classes can be greatly reduced. Therefore this column is left in for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df['Collection'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is appearent that objects that are part of the same collection are likely related to each other in some ways. However, our learning algorithm should not learn to classify the object based on that. This would essential be the same as learning to group objects simply based on the fact that people have done it previously. While that would not technically be wrong and might even be leveraged for some machine learning task, it would distract from the real goal of grouping individual images based on their contextual information. Therefore this column should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Collection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## For each row a picture and vice versa\n",
    "If everything was downloaded correctly, the image folder should only have png's in it and everyone of them should be related to a row in the dataset.\n",
    "We will also drop the rows where the images are no longer available from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rows_without_image = [row['Identifier'] for _, row in tqdm(df.iterrows(), total=df.shape[0]) if\n",
    "                      not os.path.isfile(os.path.join(image_dir, row['Identifier'] + '.png'))]\n",
    "print(f'There are {len(rows_without_image)} entries without images associated to them.')\n",
    "\n",
    "# Checking that only png files are in the directory and that every file is matched with an entry in the dataset\n",
    "# Note that this raises an Exception since the problem cannot be handled by simple data cleaning\n",
    "for f in os.listdir(image_dir):\n",
    "    assert os.path.isfile(os.path.join(image_dir, f)) and f.endswith('.png'), f'Unexpected finding (either folder or file that is not a png): {f}'\n",
    "    assert (df['Identifier'] == f[:-4]).any(), f'No matching identifier was found in the dataset for the picture {f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shape_before = df.shape\n",
    "df = df[~df['Identifier'].isin(rows_without_image)]\n",
    "print(f'Shape before: {shape_before}\\nShape after: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I previously checked that the \"ExpertsTag\" column was non empty. However, many rows only hold an empty array as an entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(df[df['ExpertTags'] == '[]']))\n",
    "print(len(df[df['Description'].isna()]))\n",
    "print(len(df[(df['ExpertTags'] == '[]') & (df['Description'].isna())]))\n",
    "print(len(df[(df['ExpertTags'] == '[]') & (df['Description'].isna()) & (df['Title'].isna())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we look at the output of the code below, we see that there are two problems with the descriptions. First, the short ones are often references to the artists life or to other works of art. Second, it is unreasonable to assume that a learner could decide what information concerns the art itself and what is just side information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0, 300, step := 100):\n",
    "    print(f'---------------- Descriptions with length {i} to {i + step} ----------------')\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(current := [entry for entry in df[~df['Description'].isna()]['Description'].unique() if\n",
    "                                  i < len(entry) < i + step])\n",
    "    print(*current[0:5], '\\n\\n\\n\\n', sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "However we also see that squared brackets are used to denote references and those entries seem to be primarly descriptions of the objects themselves. If I would filter out all the other, I would lose 425 descriptions which means that this concerns more then 10 % of what is still left of the dataset. Still, it is necessary because having no additional data is better than adding potentially misleading tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "useable = [entry for entry in df[~df['Description'].isna()]['Description'].unique() if\n",
    "           ']' in entry and 'Inv.' not in entry]\n",
    "\n",
    "print(f'{len(useable)} useable entities.\\nExamples from useable entries:')\n",
    "print(*[useable[i] for i in range(1, 101, 10)], sep='\\n\\n')\n",
    "\n",
    "\n",
    "different = [u for u in useable if not u.strip().endswith(']')]\n",
    "print(len(different))\n",
    "print(*[different[i][0:500] + '...' for i in range(1, 16, 3)], sep='\\n\\n')\n",
    "\n",
    "useable_not_unique = len([entry for entry in df[~df['Description'].isna()]['Description'] if\n",
    "           ']' in entry and 'Inv.' not in entry])\n",
    "unusable_not_unique = len(df[~df['Description'].isna()]['Description']) - useable_not_unique\n",
    "print(f'{useable_not_unique} descriptions are left from the original {useable_not_unique + unusable_not_unique} ({unusable_not_unique} were filtered out).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As can be seen from what is shown in the cell above, the 'useable' entries talk about the images themselves. The 'unusable' entries often talk about the life of the artist, etc. which is too loosely correlated with the images for our task.\n",
    "\n",
    "As can also be seen in the cell above, there are also 15 entries that contain the symbol ']' that are not in the expected format. Nevertheless they are left in, since they, for the most part, also describe the images themselves.\n",
    "\n",
    "Another problem becomes appearent here: if I generate labels from these 375 'useable' entities and later go on matching images based on their labels, these 375 images would appear significantly more often, just because they have so many more opportunities to match. At this point let it be mentioned that this actually became a problem and is discussed furhter in the notebook called 'matching'.\n",
    "\n",
    "This could be alleviated by weighing each label with 1/(number of labels for entity), but then entities with a greater number of labels would never be matched, because they require a large overlap. For example 299/500 overlap would lose against a 3/5 overlap, eventhough the former one would likely be the better candidate.\n",
    "\n",
    "I could also rank labels based on their origins, so I will add them to have this option in the future. In order to stay flexible later on, I will not save the weights directly, but instead just denote their column of origin:\n",
    "\n",
    "'Exp' - Origin from column 'ExpertTags'\n",
    "'Title' - Origin from column 'Title'\n",
    "'Des' - Origin from column 'Description'\n",
    "\n",
    "Note: Later it was decided that the NLP library 'spacy' will be used, which features a method to calculate similarity between strings. This essentially solves the problem as tags can also negatively impact the similarity. Recording the origin of the tags might come in handy later on nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_1):\n",
    "    df = pd.read_pickle(pickle_path_1)\n",
    "else:\n",
    "    df['GeneratedTags'] = ''\n",
    "\n",
    "\n",
    "    def extract_tags(cell, marker, from_cell):\n",
    "        articles = ['der', 'des', 'dem', 'den', 'die', 'das', 'des', 'ein', 'eines', 'einem', 'einen', 'einer', 'dieser',\n",
    "                    'diese', 'diesem', 'diesen', 'dieses', 'jener', 'jenes', 'jenem', 'jenen', 'er', 'sie', 'es', 'wir',\n",
    "                    'ihr']\n",
    "\n",
    "        if pd.isnull(cell):\n",
    "            return []\n",
    "        cur_tags = []\n",
    "        doc = nlp(cell)\n",
    "        for nc in doc.noun_chunks:\n",
    "            current_tag = []\n",
    "            words = nc.text.split(' ')\n",
    "            for w in words:\n",
    "                if w not in nlp.Defaults.stop_words and w.lower() not in articles and len(w.strip()) > 0:\n",
    "                    current_tag.append(w.strip())\n",
    "            if len(current_tag) > 0:\n",
    "                cur_tags.append((' '.join(current_tag), marker, from_cell))\n",
    "        return cur_tags\n",
    "\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        generated = []\n",
    "        generated += extract_tags(row['Description'], 'Des', row['Description'])\n",
    "        generated += extract_tags(row['Title'], 'Title', row['Title'])\n",
    "\n",
    "        expert_tags = [t for t in row['ExpertTags'][2:-2].split('\\', \\'') if t != '']\n",
    "\n",
    "        for tag in expert_tags:\n",
    "            generated.append((tag, 'Exp', tag))\n",
    "\n",
    "        df.loc[ind, 'GeneratedTags'] = generated\n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the ExpertTags column is now fully contained in the newly generated column, I would not need the associated column anymore.\n",
    "However, later on in the project the unprocessed Expert Tag labels are needed for lookups in the iconclass hierarchy, which they are based upon. Therefore they will be left in.\n",
    "\n",
    "I leave the Description in as well, since later, when the user is shown the images, possibly through a web-frontend, it would add to the experience if also the description was shown.\n",
    "\n",
    "I could have also included the 'Temporal' column as tags, but experiments have shown that not all of the tags would be recognized by the library.\n",
    "This would lead to the unwanted behaviour that for some work of arts the epochs are considered and for others they are not.\n",
    "Instead I will keep this column for selfsupervised learning later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_top_1000_to_file(name:str):\n",
    "    all_tags = dict()\n",
    "    for df_ind, df_row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        for gt in df_row['GeneratedTags']:\n",
    "            if gt not in all_tags.keys():\n",
    "                all_tags[gt] = [gt[0], 0]\n",
    "            all_tags[gt][1] += 1\n",
    "    sorted_tags = sorted([(all_tags[at][1], all_tags[at][0]) for at in all_tags])[::-1]\n",
    "\n",
    "    with open(name, 'w+', encoding=\"utf-8\") as file:\n",
    "        file.write('\\n'.join([st[1] for st in sorted_tags][0:1000]))\n",
    "write_top_1000_to_file('all_tags.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us also take a look at the three cells below. Three problems become appearent:\n",
    "- Tags can include other tags (like in 'Emilie' and 'Emilie von')\n",
    "    - This has to be taken care of separetely, since the words are lemmatized in the processes below which will cause more tags to contain each other again.\n",
    "- The entity recognition does not work so well on single words\n",
    "    - I need to find the entities on the whole tag text and then filter out in a seperate step\n",
    "\n",
    "Furthermore, persons are generally not that interesting to the context, Locations and Misc and Organizations on the other hand likely add more context. For that reason these are left in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[t for t in df[df['Identifier'] == '3613']['GeneratedTags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(nlp('Emilie').ents)\n",
    "print(nlp('Emilie isst einen Apfel.').ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(nlp('tschechischer').ents[0].label_)\n",
    "print(nlp(nlp.tokenizer('tschechischer')[0].lemma_).ents[0].label_)\n",
    "print(nlp('Ceska Krajiná').ents)\n",
    "print(nlp(nlp.tokenizer('Ceska Krajiná')[0].lemma_).ents[0].label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_2):\n",
    "    df = pd.read_pickle(pickle_path_2)\n",
    "else:\n",
    "    df = pd.read_pickle(pickle_path_1)\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        current_tags = []\n",
    "        for tag_tuple in row['GeneratedTags']:\n",
    "            tag = tag_tuple[0]\n",
    "\n",
    "            if tag.strip().lower() in ['bild', 'teil', 'seitenansicht', 'profil', 'gemälde', 'darstellung', 'radierungen',\n",
    "                                       'halbprofil', 'künstler', 'dreiviertelprovil', 'bild im bild', 'stichvorlage', 'etc',\n",
    "                                       'dargestellten', 'bildes', 'bilder', 'min', 'max', 'vgl', 'ausdruck', 'stillleben',\n",
    "                                       'stil', 'seite', 'url', 'szene aus']:\n",
    "                continue\n",
    "\n",
    "            for p in ['nr.', 'inv.', 'inv ']:\n",
    "                if p in tag.lower():\n",
    "                    continue\n",
    "\n",
    "            ent_list = nlp(tag).ents\n",
    "            labels = [l.label_ for l in ent_list]\n",
    "\n",
    "            to_filter = [str(e) for e in nlp(tag).ents if e.label_ == 'PER' and 'christus' not in e.label_.lower() and 'könige' not in e.label_.lower()]\n",
    "\n",
    "            temp = ' '.join([''.join([l for l in word if l.isalpha()]).strip() for word in tag.split(' ')])\n",
    "            for f in to_filter:\n",
    "                temp.replace(f, '')\n",
    "            tag = ''.join(temp)\n",
    "            current_words = ''\n",
    "            for cur_word in tag.split(\" \"):\n",
    "                word = cur_word\n",
    "                if word.lower() == 'hl' or word.lower() == 'hll':\n",
    "                    # Note that it does not matter what word ending is used, because of the later lemmatization\n",
    "                    word = 'heiliger'\n",
    "\n",
    "                word = ''.join([l for l in word if l.isalpha()]).strip()\n",
    "\n",
    "                #filter out all special characters and numbers\n",
    "                if len(word) == 0:\n",
    "                    continue\n",
    "\n",
    "                # words that are supposed to be written with a lower letter at the beginning sometimes have a capital letter\n",
    "                # at the beginning. Since these words are then not represented by the model, the lemmatization would not work\n",
    "                if sum(nlp(word).vector) == 0 and sum(nlp(word.lower()).vector) != 0:\n",
    "                    word = word.lower()\n",
    "                if word == 'Paar' or word == 'Herde':  # These words get lemmatized wrong\n",
    "                    current_words += word + ' '\n",
    "                else:\n",
    "                    current_lemma = nlp.tokenizer(word)[0].lemma_\n",
    "                    if nlp.vocab[current_lemma].rank < nlp.vocab[word].rank:\n",
    "                        current_words += current_lemma + ' '\n",
    "                    else:\n",
    "                        current_words += word + ' '\n",
    "            if len(current_words) > 0:\n",
    "                current_tags.append((current_words.strip(), tag_tuple[1], tag_tuple[2]))\n",
    "        # Because of the steps above it can happen that different words result in the same tag. Therefore we take the set.\n",
    "        df.loc[ind, 'GeneratedTags'] = list(set(current_tags))\n",
    "        \n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(df[df['Identifier'] == '1093a-p']['GeneratedTags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Words like 'Baumstudie', 'Bauerngehöft' etc. are (wrongly) filtered out as well if I filter out named entities, but it has to be considered that these are words that the model does not know anyway and therefore there is no vector representation, meaning that the similarity function would only return 0 for any comparison other than completly identical words.\n",
    "\n",
    "There exist methods where part of words are also understood, but for the most part this will not do any good in this dataset, because 'Bartholomäus' has nothing to do with 'Bart', 'Mannersdorf' nothing with 'Mann' and 'Fischerwirt' nothing with 'Fisch' (except that one could probably order fish there) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Unfortunately not all names are recognized as such and so we still have 'Emilie' in the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(nlp('Baumstudie').similarity(nlp('Baum')))\n",
    "print(nlp('Männer').similarity(nlp('Mann')))\n",
    "print(nlp(nlp.tokenizer('Männer')[0].lemma_).similarity(nlp(nlp.tokenizer('Mannes')[0].lemma_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we see the above mentioned problem: The language model cannot associate Baumstudie with any other word. However, we see a different problem as well: One would think that 'Männer' (men) and 'Mann' (man) are pretty similar to each other, but this is not reflected in the results. Likely this is because the two words rarely appear in the same sentence together. For this reason we will use lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(nlp('Katze').similarity(nlp('Futter')))\n",
    "print(nlp('Katze Napf').similarity(nlp('Futter')))\n",
    "print(nlp('Katze Baum').similarity(nlp('Futter')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the cell above it can be seen that additional words can both positively or negatively impact the similarity. While this seems obvious, it is still important to mention, since this means that entities with have more tags do not necessarily have an advantage in the matching process later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_unrepresented_and_empty_tags(for_df):\n",
    "    not_found = 0\n",
    "    empty_tags = 0\n",
    "    concerning_ids = set()\n",
    "    for df_ind, df_row in tqdm(for_df.iterrows(), total=for_df.shape[0]):\n",
    "        tup_arr = df_row['GeneratedTags']\n",
    "        for ta in tup_arr:\n",
    "            assert len(ta[0]) != 0, f'Empty tag: {df_row[\"GeneratedTags\"]}'\n",
    "            assert len(ta[1]) != 0, f'Empty tag-label: {df_row[\"GeneratedTags\"]}'\n",
    "\n",
    "            if sum(nlp(ta[0]).vector) == 0:\n",
    "                not_found += 1\n",
    "                concerning_ids.add(df_row['Identifier'])\n",
    "\n",
    "        if len(tup_arr)==0:\n",
    "            empty_tags += 1\n",
    "            concerning_ids.add(df_row['Identifier'])\n",
    "    print(f'{not_found} entities are not represented by the nlp model. '\n",
    "          f'{empty_tags} entities are without any tags. This concerns {len(concerning_ids)} artworks.')\n",
    "    return concerning_ids\n",
    "_ = count_unrepresented_and_empty_tags(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: Below we will see that the similarity function can work with entities that are not represented in the model by ignoring the unrepresented parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An open question is whether I should filter out all the tags that are not represented by a vector in the model. Let us first look at the behaviour of the similarity function in cases where unrepresented words are part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(nlp('abcd').similarity(nlp('abcd')))\n",
    "print(nlp('abcd').similarity(nlp('Abcd')))\n",
    "print(nlp('abcd Haus').similarity(nlp('abcd Garten')))\n",
    "print(nlp('Haus').similarity(nlp('abcd Garten')))\n",
    "print(nlp('Garten').similarity(nlp('abcd Garten')))\n",
    "print(nlp('Haus').similarity(nlp('Garten')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If two strings are identical, a similarity of 1 is returned, regardless of whether the word is represented in the model or not. This is also important to know since it means that unknown tags are not necessarily useless altogether. However in the great majority of cases they would be a computational burden. It would also be an unwanted behaviour that sim('abcd', 'abcd') = 1, but sim('abcd', 'Abcd') = 0. Therefore it is necessary to process these unusable words in order to find pieces that are represented by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_3) and os.path.exists(pickle_path_4) and os.path.exists(pickle_path_5):\n",
    "    df = pd.read_pickle(pickle_path_3)\n",
    "    with open(pickle_path_4, 'rb') as f:\n",
    "        total_tag_words = pickle.load(f)\n",
    "    with open(pickle_path_5, 'rb') as f:\n",
    "        removed_words = pickle.load(f)\n",
    "else:\n",
    "    split_word_cache = dict()\n",
    "    total_tag_words = 0\n",
    "    removed_words = []\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        current_tags = []\n",
    "        for tag_tuple in row[\"GeneratedTags\"]:\n",
    "            tag = tag_tuple[0]\n",
    "\n",
    "            current_words = ''\n",
    "            for cur_word in tag.split(\" \"):\n",
    "                total_tag_words += 1\n",
    "                word = cur_word\n",
    "\n",
    "                if sum(nlp(word).vector) == 0:\n",
    "                    if word in split_word_cache.keys():\n",
    "                        found = split_word_cache[word]\n",
    "                    else:\n",
    "                        found = gws.split_german_word(word)\n",
    "                        split_word_cache[word] = found\n",
    "                    if not found:\n",
    "                        removed_words.append(word)\n",
    "                    else:\n",
    "                        for f in found:\n",
    "                            current_words += f + ' '\n",
    "                else:\n",
    "                    current_words += word + ' '\n",
    "            if len(current_words) > 0:\n",
    "                current_tags.append((current_words.strip(), tag_tuple[1], tag_tuple[2]))\n",
    "        # Because of the steps above it can happen that different words result in the same tag. Therefore the set is taken.\n",
    "        df.loc[ind, 'GeneratedTags'] = list(set(current_tags))\n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_3)\n",
    "    with open(pickle_path_4, 'wb+') as f:\n",
    "        pickle.dump(total_tag_words, f)\n",
    "    with open(pickle_path_5, 'wb+') as f:\n",
    "        pickle.dump(removed_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(total_tag_words)\n",
    "print(len(removed_words))\n",
    "print(removed_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[t for t in df[df['Identifier'] == '3613']['GeneratedTags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see in the example above, some tags are fully contained in other tags. (Disregarding the fact that names should have filtered out in the steps above. The nlp library does not work perfectly on these small text snippets.)\n",
    "\n",
    "Therefore these will be filtered out. However, this is implemented in the project, so that the filtering is done dynamically. The reason behind this is that there may be a duplicate considering Title and Exp tags e.g. with tuples like ('tag', 'Exp'), ('I am a tag', 'Title'), but if only 'Exp' tags are considered there would not be a duplicate value.\n",
    "\n",
    "Note: The list that was used for filtering is from an older version of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_top_1000_to_file('all_tags_new.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filter_out = ['Georg Lechner', 'Selbstporträt ein Maler', 'mit Figur', 'Staffage', 'Sabine Grabner', 'Nr', 'Werk', 'Literatur', 'Kat',\n",
    "              'Studie', 'Künstler', 'Jahrhundert', 'Maler', 'Selbstbildnis', 'Porträt', 'Künstler', 'S', 'Vgl', 'Vordergrund', 'Studie',\n",
    "              'Eindruck', 'Skizze', 'Gemälde', 'Entwurf', 'Sammlung', 'Malerei', 'a', 'Österreichischen Galerie Belvedere verb',\n",
    "              'etc', 'Selbstporträt ein Bildhauer', 'Sabine', 'Reproduktion einer Skulptur', 'Prospekt', 'Folge', 'Selbstporträt ein Grafikers',\n",
    "              'Porträt ein Schauspieler', 'Dies', 'Brustbild', 'Bildnis', 'Beschreibung', 'zwölf Radierung', 'Skulptur',\n",
    "              'Bildnis', 'Art', 'verschieden Ansicht', 'Motiv', 'Büste', 'Titelblatt', 'Teil', 'Johann Peter Krafft', 'Bedeutung',\n",
    "              'Name', 'österreichisch Malerei', 'österreichisch Maler', 'Österreichischen Galerie', 'vorliegend Bild', 'b', 'Titel',\n",
    "              'Rahmen', 'Porträt', 'Detail', 'Bild', 'Aquarell', 'zugehörig', 'weiters', 'ich Jahr', 'ernten', 'erneuert Blendrahmen', 'd',\n",
    "              'c', 'Stelle', 'Serie', 'Fotografie', 'Datierung', 'Beispiel', 'Anlass', 'Öl', 'ich', 'Stillleben mit verwandt Gegenstand',\n",
    "              'Hinweis', 'Gegenstand', 'Überlieferung', 'Ölbild', 'vgl', 'stilistisch Grund', 'eigenhändige Radierung', 'radierung']\n",
    "\n",
    "if os.path.exists(pickle_path_6):\n",
    "    df = pd.read_pickle(pickle_path_6)\n",
    "else:\n",
    "    df = pd.read_pickle(pickle_path_3)\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        current_tags = []\n",
    "        for tag_tuple in row['GeneratedTags']:\n",
    "            tag = tag_tuple[0]\n",
    "            \n",
    "            # Single letters sometimes appear because of Titles like \"F. Gawet\"\n",
    "            if tag.strip().lower() not in filter_out and len(tag.strip()) > 1:\n",
    "                current_tags.append((tag.strip(), tag_tuple[1], tag_tuple[2]))\n",
    "        df.loc[ind, 'GeneratedTags'] = list(set(current_tags))\n",
    "        \n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagless_images = count_unrepresented_and_empty_tags(df)\n",
    "print(tagless_images, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Converting Lists to Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_7):\n",
    "    df = pd.read_pickle(pickle_path_7)\n",
    "else:\n",
    "    df = pd.read_pickle(pickle_path_6)\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        df.loc[ind, 'GeneratedTags'] = tuple(df.loc[ind, 'GeneratedTags'])\n",
    "\n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating Iconclass Tags Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_8):\n",
    "    df = pd.read_pickle(pickle_path_8)\n",
    "else:\n",
    "    df = pd.read_pickle(pickle_path_7)\n",
    "    df['IconclassTags'] = ''\n",
    "    def create_iconclass_column(error_mode):\n",
    "        ic: IconclassCache = IconclassCache.instance\n",
    "        all_tags = set()\n",
    "        for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            for e in tuple(t for t in row['ExpertTags'][2:-2].split('\\', \\'') if t != ''):\n",
    "                all_tags.add(e)\n",
    "\n",
    "        non_iconclass_tags = ic.get_all_missing_tags(all_tags=all_tags)\n",
    "\n",
    "        for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            current_tags = []\n",
    "            current_tag_tuples = [tt for tt in row['GeneratedTags']]\n",
    "\n",
    "            for tag in [t for t in row['ExpertTags'][2:-2].split('\\', \\'') if t != '']:\n",
    "                if error_mode or tag not in non_iconclass_tags:\n",
    "                    current_tags.append(tag)\n",
    "                    current_tag_tuples.append((tag, 'Icon', tag))\n",
    "            df.loc[ind, 'IconclassTags'] = list(set(current_tags))\n",
    "            df.loc[ind, 'GeneratedTags'] = current_tag_tuples\n",
    "    try:\n",
    "        create_iconclass_column(False)\n",
    "    except NumberOfTriesExceededException as e:\n",
    "        warnings.warn('The initialization of the IconclassCache was not successfull. The column \"IconclassTags\" will be made a copy of the \"ExpertTags\" column. This will affect the iconclass measure visualization.')\n",
    "        create_iconclass_column(True)\n",
    "        df.to_pickle(pickle_path_7)\n",
    "        warnings.warn('Checkpoint saved, continue this notebook manually or rerun it.')\n",
    "        raise e\n",
    "    except IconclassCacheGenerationFailedException as e:\n",
    "        warnings.warn('On the third fail, the column will be made a copy of the \"ExpertTags\" column.')\n",
    "        raise e\n",
    "\n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These new tags need to be filtered and processed as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_9):\n",
    "    df = pd.read_pickle(pickle_path_9)\n",
    "else:\n",
    "    df = pd.read_pickle(pickle_path_8)\n",
    "\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        current_tag_tuples = [t for t in row['GeneratedTags']]\n",
    "\n",
    "        new_tag_tuples = []\n",
    "\n",
    "        for tt in current_tag_tuples:\n",
    "\n",
    "            if tt[1] == 'Exp':\n",
    "                found = False\n",
    "                for compare in current_tag_tuples:\n",
    "                    if compare[1] == 'Icon' and tt[2] == compare[2]:\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    new_tag_tuples.append((tt[0], 'NotIcon', tt[2]))\n",
    "            elif tt[1] == 'Icon':\n",
    "                for compare in current_tag_tuples:\n",
    "                    if compare[1] == 'Exp' and tt[2] == compare[2]:\n",
    "                        found = True\n",
    "                        new_tag_tuples.append((compare[0], tt[1], tt[2]))\n",
    "                        break\n",
    "                continue\n",
    "\n",
    "            new_tag_tuples.append(tt)\n",
    "        df.loc[ind, 'GeneratedTags'] = new_tag_tuples\n",
    "\n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_pickle(pickle_path_9).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The ExpertTags column and the Iconclass tags column should be tuples as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_10):\n",
    "    df = pd.read_pickle(pickle_path_10)\n",
    "else:\n",
    "    df = pd.read_pickle(pickle_path_9)\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        df.loc[ind, 'ExpertTags'] = tuple(t for t in row['ExpertTags'][2:-2].split('\\', \\'') if t != '')\n",
    "        df.loc[ind, 'IconclassTags'] = tuple(df.loc[ind, 'IconclassTags'])\n",
    "\n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(pickle_path_10)\n",
    "[t for t in df[df['Identifier'] == '9388']['GeneratedTags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Add all objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving the Dataframe to pickle\n",
    "Using pickle preserves the metainformation like types and avoids errors that can happen when parsing strings.\n",
    "The downside is that the result is not a human readable file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(pickle_path_10)\n",
    "df.to_pickle('cleaned_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_config, best_number_of_epochs = get_winning_config_with_augmentations()\n",
    "\n",
    "if not os.path.exists(get_results_path('With_Augmentations', 'model_all_data')):\n",
    "    train_model_on_all_data('With_Augmentations', best_config, best_number_of_epochs)\n",
    "\n",
    "if not os.path.exists(get_results_path('Without_Augmentations', 'model_all_data')):\n",
    "    train_model_on_all_data('Without_Augmentations', *get_winning_config_without_augmentations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_11):\n",
    "    df = pd.read_pickle(pickle_path_11)\n",
    "else:\n",
    "    df = pd.read_pickle(pickle_path_10)\n",
    "    df['Objects'] = ''\n",
    "    from Project.VisualFeaturesBranches.ObjectDetection.ObjectDetectionNetworkUtils import load_model\n",
    "    from Project.VisualFeaturesBranches.ObjectDetection.ArtDataset import ArtDataset\n",
    "    from Project.VisualFeaturesBranches.ObjectDetection.ObjectDetectionNetworkUtils import reduce_box_count\n",
    "    from Project.AutoSimilarityCacheConfiguration.DataAccess import DataAccess\n",
    "\n",
    "    da: DataAccess = DataAccess.instance\n",
    "    best_experiment = 'With_Augmentations'\n",
    "    device = best_config['device']\n",
    "    best_model = load_model(best_experiment)\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    result_dict = dict()\n",
    "\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        identifier = row['Identifier']\n",
    "\n",
    "        if identifier in da.get_ids_for_which_bounding_boxes_exist():\n",
    "            labels = da.get_bounding_boxes_and_labels_for_identifier(identifier)[1]\n",
    "        else:\n",
    "            dataset = ArtDataset([identifier], 1200, 1200, 0 ,0 ,0 ,0 ,0, 0, (0, 0), 0)\n",
    "            img = dataset[0]\n",
    "            output = best_model([img[0].to(device)])\n",
    "            reduced = reduce_box_count(output[0], 0.1, 0.25, 0)\n",
    "            labels = [da.get_class_label_for_index(l) for l in reduced['labels']]\n",
    "        df.loc[ind, 'Objects'] = tuple(labels)\n",
    "\n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_11)\n",
    "    del DataAccess.instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path_12):\n",
    "    df = pd.read_pickle(pickle_path_12)\n",
    "else:\n",
    "    df = pd.read_pickle(pickle_path_11)\n",
    "\n",
    "    for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        df.loc[ind, 'GeneratedTags'] = list(row['GeneratedTags']) + list((t, 'Obj', t) for t in (row['Objects']))\n",
    "\n",
    "    # Checkpoint\n",
    "    df.to_pickle(pickle_path_12)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculating weights\n",
    "\n",
    "In order to avoid a bias towards the sources that give many noun chunks, the results also receive individual weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_weights_for_column():\n",
    "        for ind, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            current_count_dict = dict()\n",
    "\n",
    "            for tt in row['GeneratedTags']:\n",
    "                current_key = (tt[1], tt[2])\n",
    "                if current_key not in current_count_dict.keys():\n",
    "                    current_count_dict[current_key] = 1\n",
    "                else:\n",
    "                    current_count_dict[current_key] += 1\n",
    "\n",
    "            new_entries = []\n",
    "\n",
    "            for tt in row['GeneratedTags']:\n",
    "                new_entries.append((tt[0], tt[1], 1/current_count_dict[(tt[1], tt[2])]))\n",
    "\n",
    "            nr_of_obj_tags = 0\n",
    "            for e in new_entries:\n",
    "                if e[1] == 'Obj':\n",
    "                    nr_of_obj_tags += 1\n",
    "\n",
    "            # This works because every object recognized leads to exactly one tag\n",
    "            tmp = []\n",
    "            for e in new_entries:\n",
    "                if e[1] == 'Obj':\n",
    "                    tmp.append((e[0], e[1], 1 / nr_of_obj_tags))\n",
    "                else:\n",
    "                    tmp.append(e)\n",
    "            new_entries = tmp\n",
    "\n",
    "            df.loc[ind, 'GeneratedTags'] = new_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving the Dataframe to pickle and csv\n",
    "(and checking that it worked correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(pickle_path_12)\n",
    "calculate_weights_for_column()\n",
    "df.to_pickle('cleaned_dataframe.pkl')\n",
    "file_name = 'cleaned_dataframe.csv'\n",
    "pd.read_pickle('cleaned_dataframe.pkl').to_csv(file_name, index=False)\n",
    "pd.read_csv(file_name).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print([t for t in df[df['Identifier'] == '396']['GeneratedTags']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Masters-Thesis-Implementation)",
   "language": "python",
   "name": "pycharm-8f91eb99"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
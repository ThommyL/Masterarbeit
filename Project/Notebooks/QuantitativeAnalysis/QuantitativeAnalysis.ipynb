{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "from Project.AutoSimilarityCacheConfiguration.DataAccess import DataAccess\n",
    "from Project.Utils.Misc.OriginContainer import OriginContainer\n",
    "from Project.AutoSimilarityCache.Caching.SimilarityRankDatabase import SimilarityRankDatabase\n",
    "from Project.Utils.FilterCache.FilterCache import FilterCache\n",
    "from Project.Matching_and_Similarity_Tasks.Semantic_Path import generate_path\n",
    "from Project.Utils.Misc.Misc import cosine_distance\n",
    "from Project.Utils.Misc.Nlp import NLP\n",
    "from Project.Utils.TextTagProcessing.GermanWordSplitter import GermanWordSplitter\n",
    "from Project.Utils.IconclassCache.IconclassCache import IconclassCache\n",
    "\n",
    "fc: FilterCache = FilterCache.instance\n",
    "fc.set_rule_min_nr_of_tags_of_combined_origin(2, 'Icon')\n",
    "\n",
    "\n",
    "# Note in this configuration it makes no difference whether 'Obj' is considered in the OriginContainer or not\n",
    "identifiers = fc.get_filtered_identifiers(origin_container=OriginContainer(('Title', 'NotIcon', 'Obj')))\n",
    "\n",
    "N_SAMPLES = 500\n",
    "\n",
    "if os.path.exists('pairs.pkl'):\n",
    "    with open('pairs.pkl', 'rb') as f:\n",
    "        pairs = pickle.load(f)\n",
    "else:\n",
    "    pairs = []\n",
    "    da: DataAccess = DataAccess.instance\n",
    "\n",
    "    for p in tqdm(range(N_SAMPLES)):\n",
    "        while True:\n",
    "            first_index = random.randint(0, len(identifiers) - 1)\n",
    "            second_index = random.randint(0, len(identifiers) - 1)\n",
    "            while second_index == first_index:\n",
    "                second_index = random.randint(0, len(identifiers) - 1)\n",
    "            if (identifiers[first_index], identifiers[second_index]) not in pairs:\n",
    "                break\n",
    "        pairs.append((identifiers[first_index], identifiers[second_index]))\n",
    "\n",
    "    with open('pairs.pkl', 'wb+') as f:\n",
    "        pickle.dump(pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(set(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_identifiers = set()\n",
    "da: DataAccess = DataAccess.instance\n",
    "\n",
    "for p in pairs:\n",
    "    unique_identifiers.add(p[0])\n",
    "    unique_identifiers.add(p[1])\n",
    "\n",
    "print(len(unique_identifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('generated.pkl'):\n",
    "    srd: SimilarityRankDatabase = SimilarityRankDatabase.instance\n",
    "    identifiers_to_generate = set(u for u in unique_identifiers if u not in srd.get_already_generated_entries('Title&NotIcon&Obj'))\n",
    "\n",
    "    for ind, i in tqdm(enumerate(identifiers_to_generate), total=len(identifiers_to_generate)):\n",
    "        _ = srd.get_similarities_for_id(i, origin_container=OriginContainer(('Title', 'NotIcon', 'Obj')))\n",
    "\n",
    "    print('Saving, do not interrupt!')\n",
    "    srd.save_generated()\n",
    "    with open('generated.pkl', 'wb+') as f:\n",
    "        pickle.dump(None, f)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@functools.lru_cache()\n",
    "def category_to_vector(category):\n",
    "    gws: GermanWordSplitter = GermanWordSplitter()\n",
    "    nlp: NLP = NLP.instance\n",
    "    current_words = ''\n",
    "    for word in category.split(\" \"):\n",
    "        if sum(nlp.nlp(word).vector) == 0:\n",
    "            found = gws.split_german_word(word)\n",
    "\n",
    "            if found:\n",
    "                for e in found:\n",
    "                    current_words += e + ' '\n",
    "        else:\n",
    "            current_words += word + ' '\n",
    "    return nlp.nlp(current_words).vector\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_word_vector_of_average_top_most_iconclass_category(identifier):\n",
    "    data_access: DataAccess = DataAccess.instance\n",
    "    icc: IconclassCache = IconclassCache.instance\n",
    "    top_most_categories = set()\n",
    "\n",
    "    iconclass_tags = data_access.get_iconclass_tags_from_identifier(identifier)\n",
    "    assert len(iconclass_tags) > 0\n",
    "\n",
    "    for label in iconclass_tags:\n",
    "        category = icc.text_to_category(label)\n",
    "        top_most_categories.add(icc.category_to_text(icc.get_category_at_level_or_higher(category, 1)))\n",
    "\n",
    "    category_vectors = []\n",
    "    for c in top_most_categories:\n",
    "        category_vectors.append(category_to_vector(c))\n",
    "\n",
    "    final_vector = []\n",
    "    for ic in range(len(category_vectors[0])):\n",
    "        current_position = []\n",
    "        for c in category_vectors:\n",
    "            current_position.append(c[ic])\n",
    "        final_vector.append(sum(current_position) / len(current_position))\n",
    "    return tuple(final_vector)\n",
    "\n",
    "\n",
    "def get_distances(identifier, start, end):\n",
    "    assert start != end\n",
    "    vector = get_word_vector_of_average_top_most_iconclass_category(identifier)\n",
    "    distance_to_start = cosine_distance(np.array(vector),\n",
    "                                        np.array(get_word_vector_of_average_top_most_iconclass_category(start)))\n",
    "    distance_to_end = cosine_distance(np.array(vector),\n",
    "                                      np.array(get_word_vector_of_average_top_most_iconclass_category(end)))\n",
    "    return distance_to_start, distance_to_end\n",
    "\n",
    "\n",
    "def get_paths(steps, obj_tags):\n",
    "    paths = []\n",
    "    for pair in tqdm(pairs, desc=f'Generating paths with {steps} intermediate steps'):\n",
    "        paths.append(generate_path(start=pair[0], end=pair[1], intermediate_steps=steps, obj_tags=obj_tags))\n",
    "    return paths\n",
    "\n",
    "\n",
    "def get_distances_of_path(paths):\n",
    "    distances = []\n",
    "    for path in tqdm(paths, desc='Calculating distances of paths'):\n",
    "        start = path[0]\n",
    "        end = path[len(path) - 1]\n",
    "        current_distances = []\n",
    "        for generated in path:\n",
    "            current_distances.append(get_distances(generated, start, end))\n",
    "        distances.append(current_distances)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def get_orders(distances):\n",
    "    orders = []\n",
    "    for path_distances in tqdm(distances, 'Generating orders'):\n",
    "        current_ratios = dict()\n",
    "        for index, distance in enumerate(path_distances):\n",
    "            ratio = distance[0] / distance[1]\n",
    "            current_ratios[index] = ratio\n",
    "        orders.append([r[0] for r in sorted(current_ratios.items(), key=lambda x: x[1], reverse=True)])\n",
    "\n",
    "    return orders\n",
    "\n",
    "\n",
    "def get_random_paths(length, samples):\n",
    "    random_paths = []\n",
    "\n",
    "    for _ in tqdm(range(samples), desc=f'Generating random path with {length} intermediate steps'):\n",
    "        current_path = []\n",
    "        while len(current_path) != length + 2:\n",
    "            while (current_identifier := identifiers[random.randint(0, len(identifiers) - 1)]) in current_path:\n",
    "                continue\n",
    "            current_path.append(current_identifier)\n",
    "        random_paths.append(current_path)\n",
    "    return random_paths\n",
    "\n",
    "char_map = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def distance_to_string(distance):\n",
    "    return char_map[distance]\n",
    "\n",
    "def distances_to_string(distances):\n",
    "    result = ''\n",
    "    for d in distances:\n",
    "        assert isinstance(d, int)\n",
    "        result += distance_to_string(d)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_number_of_necessary_adjacent_swaps(compare: str):\n",
    "    assert len(compare) < len(char_map)\n",
    "    compare_to = ''\n",
    "\n",
    "    for current in range(0, len(compare)):\n",
    "        current_string = distance_to_string(current)\n",
    "        compare_to += current_string\n",
    "        assert current_string in compare\n",
    "    assert len(compare) == len(compare_to)\n",
    "\n",
    "    swaps = 0\n",
    "    for compare_to_position in range(len(compare_to)):\n",
    "        compare_position = None\n",
    "        for current_compare_position in range(len(compare)):\n",
    "            compare_position = current_compare_position\n",
    "            if compare[compare_position] == compare_to[compare_to_position]:\n",
    "                break\n",
    "        assert compare_position is not None\n",
    "\n",
    "        if compare_position == compare_to_position:\n",
    "            continue\n",
    "\n",
    "        swap_to_right = compare_position < compare_to_position\n",
    "        offset = 0\n",
    "\n",
    "        while compare[compare_to_position] != compare_to[compare_to_position]:\n",
    "            if swap_to_right:\n",
    "                compare = compare[:compare_position + offset] + compare[compare_position + offset + 1] + compare[\n",
    "                    compare_position + offset] + compare[compare_position + offset + 2:]\n",
    "                offset += 1\n",
    "            else:\n",
    "                compare = compare[:compare_position + offset - 1] + compare[compare_position + offset] + compare[\n",
    "                    compare_position + offset - 1] + compare[compare_position + offset + 1:]\n",
    "                offset -= 1\n",
    "            swaps += 1\n",
    "    return swaps\n",
    "\n",
    "def calculate_experiment_results(steps, experiment_name, obj_tags):\n",
    "    file_name = f'{experiment_name}_{steps}.pkl'\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    result['path'] = get_paths(steps, obj_tags)\n",
    "\n",
    "    dl_distances = []\n",
    "    current_distances = get_orders(get_distances_of_path(result['path']))\n",
    "    for d in tqdm(current_distances, desc=f'Evaluating generated paths for {file_name}'):\n",
    "        actual_result = distances_to_string(d)\n",
    "        dl_distances.append(get_number_of_necessary_adjacent_swaps(actual_result))\n",
    "    result['dl_distances'] = dl_distances\n",
    "\n",
    "    with open(file_name, 'wb+') as experiment_f:\n",
    "        pickle.dump(result, experiment_f)\n",
    "\n",
    "def calculate_random_experiment_results(steps):\n",
    "    file_name = f'random_{steps}.pkl'\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    result['random_path'] = get_random_paths(steps, 1000)\n",
    "\n",
    "    random_dl_distances = []\n",
    "    current_distances = get_orders(get_distances_of_path(result['random_path']))\n",
    "    for d in tqdm(current_distances, desc=f'Evaluating random paths for {file_name}'):\n",
    "        actual_result = distances_to_string(d)\n",
    "        random_dl_distances.append(get_number_of_necessary_adjacent_swaps(actual_result))\n",
    "    result['random_dl_distances'] = random_dl_distances\n",
    "\n",
    "    with open(file_name, 'wb+') as experiment_f:\n",
    "        pickle.dump(result, experiment_f)\n",
    "\n",
    "@functools.lru_cache\n",
    "def get_experiment_results(length, experiment_name):\n",
    "    with open(f'{experiment_name}_{length}.pkl', 'rb') as experiment_f:\n",
    "        return pickle.load(experiment_f)\n",
    "\n",
    "@functools.lru_cache\n",
    "def get_random_experiment_results(length):\n",
    "    with open(f'random_{length}.pkl', 'rb') as experiment_f:\n",
    "        return pickle.load(experiment_f)\n",
    "\n",
    "def summarize_experiment_results(length, experiment_name):\n",
    "    dl_distances = get_experiment_results(length, experiment_name)['dl_distances']\n",
    "    result = dict()\n",
    "    result['dl_distances'] = dl_distances\n",
    "    result['average_dl_distance'] = sum(dl_distances) / len(dl_distances)\n",
    "    return result\n",
    "\n",
    "def summarize_random_experiment_results(length):\n",
    "    random_dl_distances = get_random_experiment_results(length)['random_dl_distances']\n",
    "    result = dict()\n",
    "    result['random_dl_distances'] = random_dl_distances\n",
    "    result['average_random_dl_distance'] = sum(random_dl_distances) / len(random_dl_distances)\n",
    "    return result\n",
    "\n",
    "def aggregate_experiment(experiment_name, is_random, from_length, to_length):\n",
    "    aggregation_results = []\n",
    "\n",
    "    key = 'dl_distances'\n",
    "\n",
    "    if is_random:\n",
    "        key = 'random_' + key\n",
    "\n",
    "    for _ in range(len(pairs)):\n",
    "        aggregation_results.append([])\n",
    "\n",
    "    for j in range(from_length, to_length + 1):\n",
    "        summary = summarize_experiment_results(j, experiment_name)\n",
    "        normalized_dl_distances = [d / j for d in summary[key]]\n",
    "        for identifier in range(len(pairs)):\n",
    "            aggregation_results[identifier].append(normalized_dl_distances[identifier])\n",
    "\n",
    "    tmp = []\n",
    "    for r in aggregation_results:\n",
    "        tmp.append(sum(r) / len(r))\n",
    "    aggregation_results = tmp\n",
    "\n",
    "    return aggregation_results\n",
    "\n",
    "\n",
    "def get_averages_of_experiment(from_path_length, to_path_length, experiment_name, is_random):\n",
    "    averages_results = []\n",
    "    if is_random:\n",
    "        for steps in range(from_path_length, to_path_length + 1):\n",
    "            averages_results.append(summarize_random_experiment_results(steps)['average_random_dl_distance'])\n",
    "    else:\n",
    "        for steps in range(from_path_length, to_path_length + 1):\n",
    "            averages_results.append(summarize_experiment_results(steps, experiment_name)['average_dl_distance'])\n",
    "\n",
    "    return sum(averages_results) / len(averages_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for name, use_obj_tags in zip(['with_object_tags', 'without_object_tags'], [True, False]):\n",
    "    for path_length in range(3, 24):\n",
    "        if not os.path.exists(f'{name}_{path_length}.pkl'):\n",
    "            calculate_experiment_results(path_length, name, use_obj_tags)\n",
    "\n",
    "for path_length in range(3, 24):\n",
    "        if not os.path.exists(f'random_{path_length}.pkl'):\n",
    "            calculate_random_experiment_results(path_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "running_number = -1\n",
    "results = []\n",
    "\n",
    "for i in range(3, 24):\n",
    "    with_summary = summarize_experiment_results(i, 'with_object_tags')['dl_distances']\n",
    "    without_summary = summarize_experiment_results(i, 'without_object_tags')['dl_distances']\n",
    "    random_summary = summarize_random_experiment_results(i)['random_dl_distances']\n",
    "    for s in with_summary:\n",
    "        running_number += 1\n",
    "        results.append(('With Object Tags', running_number, i, s))\n",
    "    for s in without_summary:\n",
    "        running_number += 1\n",
    "        results.append(('Without Object Tags', running_number, i, s))\n",
    "    for s in random_summary:\n",
    "        running_number += 1\n",
    "        results.append(('Random', running_number, i, s))\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.xticks(np.arange(3, 24, 1))\n",
    "result_plot = sns.lineplot(data=pd.DataFrame(results, columns=['Type', 'Number', 'Intermediate Steps', 'Error']), x='Intermediate Steps', y='Error', hue='Type').set(title='Quantitative Analysis (Showing 95% Confidence Interval)')\n",
    "plt.savefig('quantitative_metric_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = dict()\n",
    "for i in range(4, 24):\n",
    "    data[i] = (get_averages_of_experiment(3, i, 'with_object_tags', False) / get_averages_of_experiment(3, i, 'without_object_tags', False) - 1) * 100\n",
    "plt.xticks(np.arange(3, 24, 1))\n",
    "sns.lineplot(data=data).set(title='Relative Difference of Errors for Paths of Length x in Percent')\n",
    "plt.savefig('quantitative_metric_scores_differences.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "absolute_differences = dict()\n",
    "for i in range(4, 24):\n",
    "    absolute_differences[i] = (get_averages_of_experiment(3, i, 'with_object_tags', False) - get_averages_of_experiment(3, i, 'without_object_tags', False))\n",
    "absolute_differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Masters-Thesis-Implementation)",
   "language": "python",
   "name": "pycharm-8f91eb99"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}